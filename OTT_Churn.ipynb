{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ee78ae7-3a30-4fb5-a3c1-08763de550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c532ca3-0815-496b-89ec-585dd65d50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"OTT_Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3db724-2b21-4c19-a16e-3294d9d1c83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Subscription_Length_Months</th>\n",
       "      <th>Monthly_Bill</th>\n",
       "      <th>Total_Usage_GB</th>\n",
       "      <th>Support_Calls</th>\n",
       "      <th>Contract_Type</th>\n",
       "      <th>Payment_Method</th>\n",
       "      <th>Has_Additional_Services</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>4596</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>124.48</td>\n",
       "      <td>225.27</td>\n",
       "      <td>2</td>\n",
       "      <td>One Year</td>\n",
       "      <td>UPI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6081</th>\n",
       "      <td>6082</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>81.14</td>\n",
       "      <td>251.83</td>\n",
       "      <td>7</td>\n",
       "      <td>Month-to-Month</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>3835</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>26.25</td>\n",
       "      <td>110.27</td>\n",
       "      <td>5</td>\n",
       "      <td>Two Year</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CustomerID  Age  Subscription_Length_Months  Monthly_Bill  \\\n",
       "4595        4596   35                          32        124.48   \n",
       "6081        6082   65                           8         81.14   \n",
       "3834        3835   50                           9         26.25   \n",
       "\n",
       "      Total_Usage_GB  Support_Calls   Contract_Type Payment_Method  \\\n",
       "4595          225.27              2        One Year            UPI   \n",
       "6081          251.83              7  Month-to-Month    Credit Card   \n",
       "3834          110.27              5        Two Year    Credit Card   \n",
       "\n",
       "      Has_Additional_Services  Churn  \n",
       "4595                        0      0  \n",
       "6081                        1      0  \n",
       "3834                        1      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8901ce6d-f15d-415a-bc3b-9b14cd1ba987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Contract_Type\n",
       "Month-to-Month    3353\n",
       "Two Year          3337\n",
       "One Year          3310\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Contract_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94066647-bb89-477b-83a5-c2a388e8b69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn\n",
       "0    7893\n",
       "1    2107\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db45d01e-433c-41f4-8c28-534fac84c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Month-to-Month':1, 'One Year':2, 'Two Year':3}\n",
    "df['Contract_Type'] = df['Contract_Type'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "515e3fef-3d0d-4d9f-b640-41c0264249ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Subscription_Length_Months</th>\n",
       "      <th>Monthly_Bill</th>\n",
       "      <th>Total_Usage_GB</th>\n",
       "      <th>Support_Calls</th>\n",
       "      <th>Contract_Type</th>\n",
       "      <th>Has_Additional_Services</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>89.68</td>\n",
       "      <td>129.26</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>80.33</td>\n",
       "      <td>297.70</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>167.31</td>\n",
       "      <td>112.11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Subscription_Length_Months  Monthly_Bill  Total_Usage_GB  \\\n",
       "2983   35                          26         89.68          129.26   \n",
       "3271   38                          36         80.33          297.70   \n",
       "809    54                          27        167.31          112.11   \n",
       "\n",
       "      Support_Calls  Contract_Type  Has_Additional_Services  Churn  \n",
       "2983              7              1                        0      1  \n",
       "3271              2              2                        0      0  \n",
       "809               8              2                        0      1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0f56dfe-ee1c-444e-8582-11cf42dea2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Subscription_Length_Months</th>\n",
       "      <th>Monthly_Bill</th>\n",
       "      <th>Total_Usage_GB</th>\n",
       "      <th>Support_Calls</th>\n",
       "      <th>Contract_Type</th>\n",
       "      <th>Has_Additional_Services</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9586</th>\n",
       "      <td>56</td>\n",
       "      <td>45</td>\n",
       "      <td>115.58</td>\n",
       "      <td>298.76</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9199</th>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>67.92</td>\n",
       "      <td>282.58</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>76.46</td>\n",
       "      <td>359.05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Subscription_Length_Months  Monthly_Bill  Total_Usage_GB  \\\n",
       "9586   56                          45        115.58          298.76   \n",
       "9199   59                          28         67.92          282.58   \n",
       "333    20                          54         76.46          359.05   \n",
       "\n",
       "      Support_Calls  Contract_Type  Has_Additional_Services  Churn  \n",
       "9586              2              3                        1      0  \n",
       "9199              3              1                        0      1  \n",
       "333               0              1                        1      0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['CustomerID','Payment_Method'])\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cee72717-f36e-442f-9a24-bd08aad0a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Age                         10000 non-null  int64  \n",
      " 1   Subscription_Length_Months  10000 non-null  int64  \n",
      " 2   Monthly_Bill                10000 non-null  float64\n",
      " 3   Total_Usage_GB              10000 non-null  float64\n",
      " 4   Support_Calls               10000 non-null  int64  \n",
      " 5   Contract_Type               10000 non-null  int64  \n",
      " 6   Has_Additional_Services     10000 non-null  int64  \n",
      " 7   Churn                       10000 non-null  int64  \n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 625.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a5f915-17fc-4e14-a874-e21511460f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of features\n",
    "features = ['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB', 'Support_Calls']\n",
    "\n",
    "# Create subplots: 2 rows  3 columns with increased size\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = axes.flatten()  # Flatten to access via a single loop\n",
    "\n",
    "# Loop through features and plot boxplots\n",
    "for i, col in enumerate(features):\n",
    "    sns.boxplot(x=df[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of {col}', fontsize=14)\n",
    "    axes[i].set_xlabel(col, fontsize=12)\n",
    "\n",
    "# Hide any unused subplot if features < total subplots\n",
    "for j in range(len(features), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "851917e9-662b-438f-83cf-6f7146b6cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Utkarsh Rai\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Utkarsh Rai\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Utkarsh Rai\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Utkarsh Rai\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i], bins=30)\n",
    "    axes[i].set_title(f'Distribution of {col} ')\n",
    "\n",
    "# Remove extra subplot if any\n",
    "for j in range(len(features), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b67fac9-3a38-478f-b390-bbaac2effa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Churn')\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc05ddc-8e6f-4e19-ad0f-689fee2eab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "408c333b-91f3-453a-b635-ba8436f375a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc558923-66c5-4e66-9ee9-1d2e7f05fa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25251704-6742-46e1-9c26-adedb29b5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "970daa22-7ddd-4202-9f58-906b9215a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def objective(trial):\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\n",
    "        \"RandomForest\", \"LogisticRegression\", \"DecisionTree\",\n",
    "        \"KNN\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"AdaBoost\"\n",
    "    ])\n",
    "\n",
    "    if classifier_name == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 50, 300),\n",
    "            max_depth=trial.suggest_int(\"rf_max_depth\", 2, 32, log=True),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"LogisticRegression\":\n",
    "        clf = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                C=trial.suggest_float(\"lr_C\", 1e-3, 1e2, log=True),\n",
    "                solver=\"liblinear\",\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"DecisionTree\":\n",
    "        clf = DecisionTreeClassifier(\n",
    "            max_depth=trial.suggest_int(\"dt_max_depth\", 2, 32, log=True),\n",
    "            min_samples_split=trial.suggest_int(\"dt_min_samples_split\", 2, 20),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"KNN\":\n",
    "        clf = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 15))\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"XGBoost\":\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 300),\n",
    "            max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 12),\n",
    "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3),\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"LightGBM\":\n",
    "        clf = LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 300),\n",
    "            max_depth=trial.suggest_int(\"lgb_max_depth\", 3, 12),\n",
    "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"CatBoost\":\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=trial.suggest_int(\"cat_iterations\", 50, 300),\n",
    "            depth=trial.suggest_int(\"cat_depth\", 3, 10),\n",
    "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3),\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif classifier_name == \"AdaBoost\":\n",
    "        clf = AdaBoostClassifier(\n",
    "            n_estimators=trial.suggest_int(\"ada_n_estimators\", 50, 300),\n",
    "            learning_rate=trial.suggest_float(\"ada_learning_rate\", 0.01, 1.0),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # Evaluate using cross-validation\n",
    "    return cross_val_score(clf, X_train, y_train, n_jobs=-1, cv=3, scoring=\"accuracy\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e250496-4769-4b39-948e-1b169c25287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 15:53:32,032] A new study created in memory with name: no-name-6860a11c-c259-4113-9c04-d789c048797e\n",
      "[I 2025-04-06 15:53:38,457] Trial 0 finished with value: 0.7881249933780783 and parameters: {'classifier': 'AdaBoost', 'ada_n_estimators': 50, 'ada_learning_rate': 0.3933830378481535}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:41,489] Trial 1 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LogisticRegression', 'lr_C': 0.36322979303226366}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:45,147] Trial 2 finished with value: 0.7441231792762588 and parameters: {'classifier': 'XGBoost', 'xgb_n_estimators': 176, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.24435451887652274}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:51,527] Trial 3 finished with value: 0.7511231951219902 and parameters: {'classifier': 'LightGBM', 'lgb_n_estimators': 276, 'lgb_max_depth': 5, 'lgb_learning_rate': 0.19218061730861608}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:51,616] Trial 4 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LogisticRegression', 'lr_C': 0.01909235997576597}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:51,672] Trial 5 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LogisticRegression', 'lr_C': 0.04927318073113191}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:51,882] Trial 6 finished with value: 0.7809994774659169 and parameters: {'classifier': 'XGBoost', 'xgb_n_estimators': 119, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.18796000383951944}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:52,839] Trial 7 finished with value: 0.7457487731512931 and parameters: {'classifier': 'XGBoost', 'xgb_n_estimators': 222, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.1994163452662997}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:54,271] Trial 8 finished with value: 0.7881249933780783 and parameters: {'classifier': 'RandomForest', 'rf_n_estimators': 212, 'rf_max_depth': 2}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:54,439] Trial 9 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LogisticRegression', 'lr_C': 0.015263901086313338}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:55,269] Trial 10 finished with value: 0.7881249933780783 and parameters: {'classifier': 'AdaBoost', 'ada_n_estimators': 51, 'ada_learning_rate': 0.3725492020103265}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:55,653] Trial 11 finished with value: 0.7607499925225025 and parameters: {'classifier': 'KNN', 'knn_n_neighbors': 7}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:58,451] Trial 12 finished with value: 0.7881249933780783 and parameters: {'classifier': 'AdaBoost', 'ada_n_estimators': 281, 'ada_learning_rate': 0.8280197590473891}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:53:58,680] Trial 13 finished with value: 0.6642492550396691 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 23, 'dt_min_samples_split': 5}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:54:02,717] Trial 14 finished with value: 0.7863746964112607 and parameters: {'classifier': 'CatBoost', 'cat_iterations': 280, 'cat_depth': 7, 'cat_learning_rate': 0.05345347482385866}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:54:03,433] Trial 15 finished with value: 0.7881249933780783 and parameters: {'classifier': 'AdaBoost', 'ada_n_estimators': 72, 'ada_learning_rate': 0.02186916349272211}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:54:05,437] Trial 16 finished with value: 0.7878750246241726 and parameters: {'classifier': 'CatBoost', 'cat_iterations': 55, 'cat_depth': 3, 'cat_learning_rate': 0.28613776619737186}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:54:06,641] Trial 17 finished with value: 0.7847499932725964 and parameters: {'classifier': 'RandomForest', 'rf_n_estimators': 60, 'rf_max_depth': 28}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:54:09,571] Trial 18 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LightGBM', 'lgb_n_estimators': 55, 'lgb_max_depth': 12, 'lgb_learning_rate': 0.02038987083556168}. Best is trial 0 with value: 0.7881249933780783.\n",
      "[I 2025-04-06 15:54:09,639] Trial 19 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 18}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:09,700] Trial 20 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:09,762] Trial 21 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:09,822] Trial 22 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:09,897] Trial 23 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:09,968] Trial 24 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,030] Trial 25 finished with value: 0.7876248683468204 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 4, 'dt_min_samples_split': 14}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,098] Trial 26 finished with value: 0.7876248683468204 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 4, 'dt_min_samples_split': 15}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,159] Trial 27 finished with value: 0.7878749308624494 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 3, 'dt_min_samples_split': 16}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,597] Trial 28 finished with value: 0.7846248213721972 and parameters: {'classifier': 'KNN', 'knn_n_neighbors': 15}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,692] Trial 29 finished with value: 0.7599991955244154 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 10, 'dt_min_samples_split': 17}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,773] Trial 30 finished with value: 0.7748743991397175 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 7, 'dt_min_samples_split': 9}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,829] Trial 31 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,887] Trial 32 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:10,953] Trial 33 finished with value: 0.7878749308624494 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 3, 'dt_min_samples_split': 18}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:11,023] Trial 34 finished with value: 0.7878749308624494 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 3, 'dt_min_samples_split': 18}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:13,978] Trial 35 finished with value: 0.775999727340909 and parameters: {'classifier': 'LightGBM', 'lgb_n_estimators': 146, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.2970384031525064}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:14,043] Trial 36 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 12}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:18,023] Trial 37 finished with value: 0.7877499464854966 and parameters: {'classifier': 'RandomForest', 'rf_n_estimators': 297, 'rf_max_depth': 11}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:18,365] Trial 38 finished with value: 0.7880000090011254 and parameters: {'classifier': 'XGBoost', 'xgb_n_estimators': 288, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.015874635432918033}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:26,879] Trial 39 finished with value: 0.7774994461026205 and parameters: {'classifier': 'CatBoost', 'cat_iterations': 134, 'cat_depth': 10, 'cat_learning_rate': 0.22710276006540736}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:26,935] Trial 40 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LogisticRegression', 'lr_C': 58.39401322096702}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:26,995] Trial 41 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:27,056] Trial 42 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 18}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:27,294] Trial 43 finished with value: 0.7878749308624494 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 3, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:27,657] Trial 44 finished with value: 0.7225006007782411 and parameters: {'classifier': 'KNN', 'knn_n_neighbors': 3}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:27,720] Trial 45 finished with value: 0.7878749308624494 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 3, 'dt_min_samples_split': 18}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:28,026] Trial 46 finished with value: 0.7854999932960367 and parameters: {'classifier': 'XGBoost', 'xgb_n_estimators': 91, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.058460501423304154}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:28,088] Trial 47 finished with value: 0.7882499777550311 and parameters: {'classifier': 'DecisionTree', 'dt_max_depth': 2, 'dt_min_samples_split': 20}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:28,143] Trial 48 finished with value: 0.7881249933780783 and parameters: {'classifier': 'LogisticRegression', 'lr_C': 90.28427877627026}. Best is trial 19 with value: 0.7882499777550311.\n",
      "[I 2025-04-06 15:54:30,200] Trial 49 finished with value: 0.7881249933780783 and parameters: {'classifier': 'AdaBoost', 'ada_n_estimators': 205, 'ada_learning_rate': 0.9256590982345239}. Best is trial 19 with value: 0.7882499777550311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Trial:\n",
      "Accuracy: 0.7882499777550311\n",
      "Params:\n",
      "  classifier: DecisionTree\n",
      "  dt_max_depth: 2\n",
      "  dt_min_samples_split: 18\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best result\n",
    "print(\" Best Trial:\")\n",
    "print(f\"Accuracy: {study.best_trial.value}\")\n",
    "print(\"Params:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20d0494a-2a7e-4968-b5de-93c5d99d9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 15:59:12,294] A new study created in memory with name: no-name-7eb0defc-1507-4829-a3db-b5d48d4052b6\n",
      "[I 2025-04-06 15:59:12,422] Trial 0 finished with value: 0.778625 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 19, 'max_features': 'log2'}. Best is trial 0 with value: 0.778625.\n",
      "[I 2025-04-06 15:59:12,491] Trial 1 finished with value: 0.788 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 17, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:12,779] Trial 2 finished with value: 0.7785 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 15, 'max_features': None}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:12,855] Trial 3 finished with value: 0.782 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:13,341] Trial 4 finished with value: 0.727125 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 21, 'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:13,500] Trial 5 finished with value: 0.78125 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 28, 'min_samples_split': 11, 'min_samples_leaf': 19, 'max_features': 'log2'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:13,597] Trial 6 finished with value: 0.7865 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:13,676] Trial 7 finished with value: 0.784375 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:14,134] Trial 8 finished with value: 0.747125 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 15, 'max_features': None}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:14,214] Trial 9 finished with value: 0.786875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 30, 'min_samples_split': 14, 'min_samples_leaf': 19, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:14,328] Trial 10 finished with value: 0.7719999999999999 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 23, 'min_samples_split': 19, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.788.\n",
      "[I 2025-04-06 15:59:14,429] Trial 11 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 16, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:14,531] Trial 12 finished with value: 0.787125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 23, 'min_samples_split': 15, 'min_samples_leaf': 15, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:14,630] Trial 13 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 17, 'min_samples_split': 20, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:14,731] Trial 14 finished with value: 0.786875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:14,829] Trial 15 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 26, 'min_samples_split': 17, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:14,985] Trial 16 finished with value: 0.787875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 19, 'min_samples_split': 14, 'min_samples_leaf': 17, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,084] Trial 17 finished with value: 0.78575 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 14, 'min_samples_split': 18, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,188] Trial 18 finished with value: 0.787875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 27, 'min_samples_split': 16, 'min_samples_leaf': 17, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,291] Trial 19 finished with value: 0.7873749999999999 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 24, 'min_samples_split': 13, 'min_samples_leaf': 13, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,423] Trial 20 finished with value: 0.747625 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 16, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,519] Trial 21 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 17, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,613] Trial 22 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 26, 'min_samples_split': 17, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,703] Trial 23 finished with value: 0.787875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 29, 'min_samples_split': 18, 'min_samples_leaf': 17, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,795] Trial 24 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 18, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:15,890] Trial 25 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 25, 'min_samples_split': 16, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,005] Trial 26 finished with value: 0.78775 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 30, 'min_samples_split': 19, 'min_samples_leaf': 14, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,150] Trial 27 finished with value: 0.7869999999999999 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 22, 'min_samples_split': 16, 'min_samples_leaf': 16, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,280] Trial 28 finished with value: 0.78425 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 20, 'min_samples_leaf': 18, 'max_features': None}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,419] Trial 29 finished with value: 0.784375 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,525] Trial 30 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 26, 'min_samples_split': 18, 'min_samples_leaf': 18, 'max_features': 'log2'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,620] Trial 31 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 17, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,712] Trial 32 finished with value: 0.786875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 29, 'min_samples_split': 15, 'min_samples_leaf': 19, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,806] Trial 33 finished with value: 0.7869999999999999 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 19, 'min_samples_split': 17, 'min_samples_leaf': 16, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:16,900] Trial 34 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 30, 'min_samples_split': 19, 'min_samples_leaf': 18, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:17,000] Trial 35 finished with value: 0.788125 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 14, 'min_samples_split': 14, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:17,483] Trial 36 finished with value: 0.758875 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 28, 'min_samples_split': 12, 'min_samples_leaf': 19, 'max_features': None}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:17,644] Trial 37 finished with value: 0.7869999999999999 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 16, 'max_features': 'log2'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:17,801] Trial 38 finished with value: 0.788125 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 2, 'min_samples_split': 18, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:17,956] Trial 39 finished with value: 0.786875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 31, 'min_samples_split': 15, 'min_samples_leaf': 19, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,161] Trial 40 finished with value: 0.78575 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 14, 'max_features': 'log2'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,287] Trial 41 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 26, 'min_samples_split': 17, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,444] Trial 42 finished with value: 0.786875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 27, 'min_samples_split': 17, 'min_samples_leaf': 19, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,594] Trial 43 finished with value: 0.788125 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 21, 'min_samples_split': 19, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,725] Trial 44 finished with value: 0.788125 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 12, 'min_samples_split': 16, 'min_samples_leaf': 18, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,872] Trial 45 finished with value: 0.759375 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 24, 'min_samples_split': 20, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:18,990] Trial 46 finished with value: 0.787875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 28, 'min_samples_split': 15, 'min_samples_leaf': 17, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:19,113] Trial 47 finished with value: 0.786875 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 15, 'min_samples_split': 13, 'min_samples_leaf': 19, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:19,332] Trial 48 finished with value: 0.744125 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 18, 'min_samples_split': 19, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 11 with value: 0.788125.\n",
      "[I 2025-04-06 15:59:19,450] Trial 49 finished with value: 0.788125 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 31, 'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.788125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 16, 'min_samples_leaf': 20, 'max_features': 'sqrt'}\n",
      "Best CV Accuracy: 0.788125\n",
      "Test Accuracy: 0.794\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "        'splitter': trial.suggest_categorical('splitter', ['best', 'random']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "    }\n",
    "    \n",
    "    clf = DecisionTreeClassifier(**params, random_state=42)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, timeout=300)  # 50 trials or 5 minutes\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\nBest Parameters:\", study.best_params)\n",
    "print(\"Best CV Accuracy:\", study.best_value)\n",
    "\n",
    "# Train final model on best parameters\n",
    "best_model = DecisionTreeClassifier(**study.best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Test accuracy\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43e03f3e-35aa-41d6-8050-3564f33da590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"ott_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924b398-0bdd-4b10-af78-3a0be594b65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071171a-8169-41b1-aa57-a2cfc05d7875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e074a06-fcc6-430e-a8f6-95486e38eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 16:09:36,849] A new study created in memory with name: no-name-7681b13f-eaeb-4c9f-bcc6-6e3b4171f9e2\n",
      "[I 2025-04-06 16:09:37,776] Trial 0 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 850, 'learning_rate': 0.017676716055168608, 'max_depth': 13, 'min_child_weight': 9, 'subsample': 0.9663771575819574, 'colsample_bytree': 0.7259082143610107, 'gamma': 2.7094535790870773, 'lambda': 0.33432039896305554, 'alpha': 0.8295179099915018}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:38,459] Trial 1 finished with value: 0.21199999999999997 and parameters: {'n_estimators': 600, 'learning_rate': 0.12367186555544782, 'max_depth': 11, 'min_child_weight': 10, 'subsample': 0.5142302909908647, 'colsample_bytree': 0.8010346341315276, 'gamma': 1.645308811510563, 'lambda': 3.7244830877608583, 'alpha': 2.1851837907674487}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:39,111] Trial 2 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 750, 'learning_rate': 0.012077046655055849, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7816063020592335, 'colsample_bytree': 0.5273080440139326, 'gamma': 2.9600007585000148, 'lambda': 2.6585610536343425, 'alpha': 4.545831907149415}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:39,378] Trial 3 finished with value: 0.20699999999999996 and parameters: {'n_estimators': 200, 'learning_rate': 0.16274562970977055, 'max_depth': 14, 'min_child_weight': 3, 'subsample': 0.7698506974935873, 'colsample_bytree': 0.9239605865789174, 'gamma': 1.255148357942197, 'lambda': 1.580841562843247, 'alpha': 3.170906454525892}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:40,074] Trial 4 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 850, 'learning_rate': 0.09337496040083977, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.6698403094299663, 'colsample_bytree': 0.8041970374276475, 'gamma': 4.140107538238599, 'lambda': 4.089696163068227, 'alpha': 3.084449186223539}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:40,303] Trial 5 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 100, 'learning_rate': 0.025888417672724727, 'max_depth': 8, 'min_child_weight': 9, 'subsample': 0.5786388579389368, 'colsample_bytree': 0.8007341638073642, 'gamma': 0.619556075544278, 'lambda': 4.329955443514734, 'alpha': 0.5323198693701386}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:41,060] Trial 6 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 900, 'learning_rate': 0.025144152184640074, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.7932239068215432, 'colsample_bytree': 0.6285976713889352, 'gamma': 2.9871230695129736, 'lambda': 2.422197846497067, 'alpha': 2.6167151620863187}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:41,677] Trial 7 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 750, 'learning_rate': 0.07368850195273653, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.774836051395093, 'colsample_bytree': 0.8732571322272935, 'gamma': 3.528363417865698, 'lambda': 0.9783484963317884, 'alpha': 1.37002387046874}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:42,253] Trial 8 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 600, 'learning_rate': 0.025257727514004343, 'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9878889741786799, 'colsample_bytree': 0.8037765224139478, 'gamma': 2.34860460154772, 'lambda': 0.8410888253805154, 'alpha': 2.08332635681752}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:42,774] Trial 9 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 550, 'learning_rate': 0.09681318420382753, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.5988220995530723, 'colsample_bytree': 0.8817569518195203, 'gamma': 3.3500050801336, 'lambda': 0.7205352966182355, 'alpha': 2.6913099298896856}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:43,702] Trial 10 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 1000, 'learning_rate': 0.27986414594521214, 'max_depth': 15, 'min_child_weight': 7, 'subsample': 0.9463449496286479, 'colsample_bytree': 0.6611240437790606, 'gamma': 4.916572364281668, 'lambda': 0.007733616756648409, 'alpha': 0.015125708688455752}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:44,430] Trial 11 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 700, 'learning_rate': 0.010056657210965287, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.8655059114223577, 'colsample_bytree': 0.5062629172059689, 'gamma': 2.297142706398275, 'lambda': 2.910959921136276, 'alpha': 4.689508408865725}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:45,018] Trial 12 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 400, 'learning_rate': 0.01011231581175188, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8808860618395328, 'colsample_bytree': 0.5006765196685475, 'gamma': 1.7982074266182755, 'lambda': 2.4965498882424373, 'alpha': 4.692082037932206}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:46,016] Trial 13 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 1000, 'learning_rate': 0.016704684541902628, 'max_depth': 12, 'min_child_weight': 6, 'subsample': 0.6807473093333624, 'colsample_bytree': 0.6600761457660906, 'gamma': 2.8441466068248333, 'lambda': 3.141326004562678, 'alpha': 3.857822577685675}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:46,801] Trial 14 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 800, 'learning_rate': 0.04011825967176813, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.8823297237917254, 'colsample_bytree': 0.5931715869311859, 'gamma': 3.998180521732017, 'lambda': 1.8138780341751355, 'alpha': 1.3785208667316242}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:47,730] Trial 15 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 400, 'learning_rate': 0.015451364369103274, 'max_depth': 11, 'min_child_weight': 5, 'subsample': 0.9996293014728208, 'colsample_bytree': 0.98499919345775, 'gamma': 0.1710481436477589, 'lambda': 1.7905321262822391, 'alpha': 1.2859046791587887}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:48,539] Trial 16 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 900, 'learning_rate': 0.04480982096598681, 'max_depth': 5, 'min_child_weight': 8, 'subsample': 0.701470252199364, 'colsample_bytree': 0.720393647246071, 'gamma': 4.173086093797132, 'lambda': 3.4643118595177644, 'alpha': 3.7518246311794576}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:49,267] Trial 17 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 700, 'learning_rate': 0.0159953990187837, 'max_depth': 14, 'min_child_weight': 5, 'subsample': 0.8454475129828845, 'colsample_bytree': 0.5741639372964926, 'gamma': 2.6117969337759117, 'lambda': 4.902862314703926, 'alpha': 3.8063434815393213}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:49,792] Trial 18 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 400, 'learning_rate': 0.03239902235893859, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.9414545758582471, 'colsample_bytree': 0.7230875005050914, 'gamma': 1.9595404786346393, 'lambda': 0.4009507599348714, 'alpha': 0.7613152993711532}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:50,551] Trial 19 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 650, 'learning_rate': 0.01776046221617749, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.817755619076916, 'colsample_bytree': 0.581170838158801, 'gamma': 1.1911258256119401, 'lambda': 1.288089593195153, 'alpha': 1.8482753983400975}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:51,058] Trial 20 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 500, 'learning_rate': 0.011177369272802558, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.7310146837320562, 'colsample_bytree': 0.5379407189946879, 'gamma': 3.5031698776011737, 'lambda': 1.9797541330089108, 'alpha': 4.973354728371259}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:51,829] Trial 21 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 850, 'learning_rate': 0.062430629331170676, 'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.6415330008114121, 'colsample_bytree': 0.7573554360441206, 'gamma': 4.89998721870154, 'lambda': 4.326978694997058, 'alpha': 3.1681591132822695}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:52,539] Trial 22 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 800, 'learning_rate': 0.1873256300608792, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.6583626723849563, 'colsample_bytree': 0.8443143099244687, 'gamma': 4.290200820842797, 'lambda': 4.036287285437376, 'alpha': 4.194857675068582}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:53,348] Trial 23 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 900, 'learning_rate': 0.07950817392744991, 'max_depth': 13, 'min_child_weight': 2, 'subsample': 0.7217026555510496, 'colsample_bytree': 0.6902394485265778, 'gamma': 3.142343640558628, 'lambda': 2.8308838059048655, 'alpha': 3.186035858184614}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:54,083] Trial 24 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 800, 'learning_rate': 0.05035860372016428, 'max_depth': 15, 'min_child_weight': 4, 'subsample': 0.609091679193583, 'colsample_bytree': 0.7520515307398757, 'gamma': 3.9178397195600576, 'lambda': 4.955107261566237, 'alpha': 4.17444257173869}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:54,945] Trial 25 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 950, 'learning_rate': 0.013197412081238506, 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.5188998883732225, 'colsample_bytree': 0.9320652034543162, 'gamma': 4.535891042802789, 'lambda': 2.2634743798589323, 'alpha': 2.963289124902399}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:55,636] Trial 26 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 750, 'learning_rate': 0.02073397962723368, 'max_depth': 13, 'min_child_weight': 5, 'subsample': 0.9266031189019672, 'colsample_bytree': 0.6286117891267996, 'gamma': 3.6845322060420673, 'lambda': 3.4477210777697227, 'alpha': 3.5583642749852715}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:56,422] Trial 27 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 850, 'learning_rate': 0.10703927366806307, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.8264912369358995, 'colsample_bytree': 0.8253033539304819, 'gamma': 2.716746500773449, 'lambda': 4.49585562909261, 'alpha': 1.6490675101508916}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:57,285] Trial 28 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 700, 'learning_rate': 0.03449195076553313, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.5584466638883148, 'colsample_bytree': 0.7813931343496858, 'gamma': 2.2388858772033835, 'lambda': 3.8462478671523925, 'alpha': 0.7038895951516849}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:57,850] Trial 29 finished with value: 0.20850000000000002 and parameters: {'n_estimators': 500, 'learning_rate': 0.169185732001246, 'max_depth': 12, 'min_child_weight': 8, 'subsample': 0.7449040148597612, 'colsample_bytree': 0.6970488180826011, 'gamma': 1.3709470836798698, 'lambda': 1.3546187069638824, 'alpha': 2.3080079875247184}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:58,454] Trial 30 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 600, 'learning_rate': 0.13536331384236192, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.6362695256737128, 'colsample_bytree': 0.8551753208396085, 'gamma': 4.566639203763468, 'lambda': 3.421605305776097, 'alpha': 4.287126099289102}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:58,732] Trial 31 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 100, 'learning_rate': 0.024256263278195113, 'max_depth': 7, 'min_child_weight': 9, 'subsample': 0.5650178704873621, 'colsample_bytree': 0.7890900424634755, 'gamma': 0.6320256787274506, 'lambda': 4.464091324274097, 'alpha': 0.12655816356702054}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:59,096] Trial 32 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 200, 'learning_rate': 0.01341574288548665, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.5456049939976564, 'colsample_bytree': 0.9352555168918399, 'gamma': 0.5600445526105138, 'lambda': 3.987751284236106, 'alpha': 0.5893076823478349}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:59,486] Trial 33 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 250, 'learning_rate': 0.028140552704613363, 'max_depth': 8, 'min_child_weight': 10, 'subsample': 0.5919201150833902, 'colsample_bytree': 0.8160824214846328, 'gamma': 3.0732210637031807, 'lambda': 4.294578253315226, 'alpha': 1.0101254216260243}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:09:59,831] Trial 34 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 100, 'learning_rate': 0.0129120377966564, 'max_depth': 14, 'min_child_weight': 4, 'subsample': 0.6805513100870184, 'colsample_bytree': 0.7674280459418665, 'gamma': 1.5746899440206683, 'lambda': 4.688273306532667, 'alpha': 0.5138055973575191}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:00,478] Trial 35 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 300, 'learning_rate': 0.021447919938126156, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.7806849040114182, 'colsample_bytree': 0.8983188746870397, 'gamma': 0.9473727255640951, 'lambda': 2.8373700861686233, 'alpha': 0.28618434329157605}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:01,608] Trial 36 finished with value: 0.20699999999999996 and parameters: {'n_estimators': 900, 'learning_rate': 0.061664433027535256, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.521133510445715, 'colsample_bytree': 0.8339433296349734, 'gamma': 2.1325898611215894, 'lambda': 3.6664246995581893, 'alpha': 0.9979117053819899}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:02,662] Trial 37 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 950, 'learning_rate': 0.08151439968209892, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.6202333510066421, 'colsample_bytree': 0.9841842780784416, 'gamma': 3.3244615228467365, 'lambda': 2.197994811819946, 'alpha': 2.0602517615599942}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:03,529] Trial 38 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 750, 'learning_rate': 0.019438623349972185, 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.7611865116004284, 'colsample_bytree': 0.7297667137714987, 'gamma': 2.5089841629872285, 'lambda': 0.09416277314887422, 'alpha': 2.5113860864951563}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:04,229] Trial 39 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 650, 'learning_rate': 0.029828003147750737, 'max_depth': 15, 'min_child_weight': 8, 'subsample': 0.9224395642085942, 'colsample_bytree': 0.890243088621778, 'gamma': 2.886353041487681, 'lambda': 1.113047095738224, 'alpha': 1.7006379893693286}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:05,053] Trial 40 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 850, 'learning_rate': 0.29566232722441776, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.7969149842433703, 'colsample_bytree': 0.8032457675969815, 'gamma': 3.711568350261791, 'lambda': 0.6084398902355659, 'alpha': 2.707999914556901}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:06,049] Trial 41 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 1000, 'learning_rate': 0.03760097369792444, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.7048718596343704, 'colsample_bytree': 0.6139897762968939, 'gamma': 3.094868149843287, 'lambda': 3.2209673514684196, 'alpha': 3.433737800927831}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:07,650] Trial 42 finished with value: 0.20750000000000002 and parameters: {'n_estimators': 950, 'learning_rate': 0.02286442437298237, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.7831431093041086, 'colsample_bytree': 0.5462128610267668, 'gamma': 0.0364463324579849, 'lambda': 2.5526638570119626, 'alpha': 2.314320291060178}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:08,509] Trial 43 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 750, 'learning_rate': 0.014803205767347909, 'max_depth': 12, 'min_child_weight': 3, 'subsample': 0.6690424881542213, 'colsample_bytree': 0.6221943090904367, 'gamma': 1.9427973047685931, 'lambda': 1.5572809494709625, 'alpha': 2.8407456995828047}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:09,380] Trial 44 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 850, 'learning_rate': 0.026568936048690685, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.9786750719365397, 'colsample_bytree': 0.6543677178984291, 'gamma': 2.457549270546358, 'lambda': 2.6464967909693264, 'alpha': 0.2964685981270021}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:10,283] Trial 45 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 900, 'learning_rate': 0.011332401746357352, 'max_depth': 11, 'min_child_weight': 4, 'subsample': 0.8870401878243412, 'colsample_bytree': 0.6856117196596447, 'gamma': 3.2533326736517694, 'lambda': 4.1324858222160445, 'alpha': 1.134323782466558}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:11,057] Trial 46 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 800, 'learning_rate': 0.018377129793928727, 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.8169035050880387, 'colsample_bytree': 0.5219743111246554, 'gamma': 2.8826417809432354, 'lambda': 3.098189727398892, 'alpha': 4.540270477098752}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:11,470] Trial 47 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 350, 'learning_rate': 0.10283853557580315, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.8581653096864093, 'colsample_bytree': 0.5609825169180518, 'gamma': 3.5026071416196194, 'lambda': 3.7821579780013765, 'alpha': 1.479468329663392}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:12,134] Trial 48 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 550, 'learning_rate': 0.04109677849031045, 'max_depth': 8, 'min_child_weight': 9, 'subsample': 0.9041598840690487, 'colsample_bytree': 0.8632552414963253, 'gamma': 1.6412929020678027, 'lambda': 0.3182511035122353, 'alpha': 2.525948689320969}. Best is trial 0 with value: 0.20599999999999996.\n",
      "[I 2025-04-06 16:10:12,778] Trial 49 finished with value: 0.20599999999999996 and parameters: {'n_estimators': 650, 'learning_rate': 0.015411519408090754, 'max_depth': 14, 'min_child_weight': 3, 'subsample': 0.7448278624300616, 'colsample_bytree': 0.6487551225392649, 'gamma': 3.976060238505161, 'lambda': 2.2827087638626997, 'alpha': 2.047735438985675}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 850, 'learning_rate': 0.017676716055168608, 'max_depth': 13, 'min_child_weight': 9, 'subsample': 0.9663771575819574, 'colsample_bytree': 0.7259082143610107, 'gamma': 2.7094535790870773, 'lambda': 0.33432039896305554, 'alpha': 0.8295179099915018}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna to optimize XGBClassifier.\"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=50),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0, 5),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 5),\n",
    "        \"objective\": \"binary:logistic\",  # Use 'multi:softmax' for multi-class classification\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"use_label_encoder\": False\n",
    "    }\n",
    "\n",
    "    # Train XGBClassifier with suggested parameters\n",
    "    model = xgb.XGBClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    return 1 - accuracy  # Minimize error (maximize accuracy)\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize classification error\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get best parameters\n",
    "print(f\"Best parameters: {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c05234fc-c819-4836-bddc-c007554a8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 0.7940\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model with optimized parameters\n",
    "final_model = xgb.XGBClassifier(**best_params, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Final Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61183b91-0c02-45b4-824a-0f35d26b2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"ott_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4a6b034-b87f-437b-bc94-0b744d15f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.4)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "    --------------------------------------- 0.0/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 525.1 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.5 MB 516.7 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 583.1 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.2/1.5 MB 655.6 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.5 MB 684.6 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.5 MB 684.6 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.3/1.5 MB 700.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 716.8 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 711.4 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.4/1.5 MB 725.3 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.5/1.5 MB 720.0 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 715.5 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.5/1.5 MB 727.4 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.6/1.5 MB 722.8 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 704.5 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 713.9 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 700.8 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.7/1.5 MB 698.2 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.7/1.5 MB 696.1 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.7/1.5 MB 696.1 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 673.6 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 673.7 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 655.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 647.3 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 647.6 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 640.0 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 640.0 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 619.3 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 607.2 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 607.2 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 607.2 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 579.1 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 579.1 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 564.5 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 545.4 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 545.4 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 537.8 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 531.0 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 531.0 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 531.0 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 503.1 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 494.1 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 494.1 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 477.6 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 477.6 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 473.3 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 466.3 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 464.5 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 464.5 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.5 MB 458.0 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.5 MB 450.6 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 453.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 447.4 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 443.4 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 443.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 435.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 434.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 434.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.5 MB 429.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 427.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 427.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 422.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 422.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 417.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 417.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 417.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 398.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 401.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 401.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 394.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 390.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 390.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 389.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 385.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 384.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 384.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.5 MB 382.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.5 MB 380.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 381.2 kB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2da95c50-f10a-4679-822a-a094b2ab5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_error\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 50),\n",
    "        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "    }\n",
    "\n",
    "    # Train LightGBM model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_metric=\"logloss\", eval_set=[(X_test, y_test)], callbacks=[early_stopping(10), log_evaluation(1)])\n",
    "\n",
    "    # Predict & evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    return 1 - accuracy  # Optuna minimizes the objective, so we return (1 - accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "684a47b4-ba57-4a08-8c15-9fc01d049ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 19:25:43,850] A new study created in memory with name: no-name-339e6677-6464-4568-b745-fd04bcaac8d1\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,350] Trial 0 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.17796508323626273, 'num_leaves': 27, 'max_depth': 5, 'min_data_in_leaf': 19, 'feature_fraction': 0.7416257918808059, 'bagging_fraction': 0.972406136370964, 'bagging_freq': 2, 'lambda_l1': 0.06567524024164581, 'lambda_l2': 0.1002980215328052}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,408] Trial 1 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.05986671363284789, 'num_leaves': 100, 'max_depth': 10, 'min_data_in_leaf': 26, 'feature_fraction': 0.6476435420204707, 'bagging_fraction': 0.9376879123977488, 'bagging_freq': 7, 'lambda_l1': 0.0008969452027075971, 'lambda_l2': 1.9729628638536693e-08}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,461] Trial 2 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.05693030748857539, 'num_leaves': 69, 'max_depth': 8, 'min_data_in_leaf': 30, 'feature_fraction': 0.8561502879363431, 'bagging_fraction': 0.8948539627930666, 'bagging_freq': 4, 'lambda_l1': 0.02097450380407092, 'lambda_l2': 0.0006178813464274346}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509847\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.510392\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.510789\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.511116\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.511337\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.511423\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.511921\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.512234\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.512518\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.512828\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.513298\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509847\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6476435420204707, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6476435420204707\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008969452027075971, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008969452027075971\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9729628638536693e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9729628638536693e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9376879123977488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9376879123977488\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6476435420204707, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6476435420204707\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008969452027075971, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008969452027075971\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9729628638536693e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9729628638536693e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9376879123977488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9376879123977488\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6476435420204707, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6476435420204707\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008969452027075971, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008969452027075971\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9729628638536693e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9729628638536693e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9376879123977488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9376879123977488\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508588\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508374\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.508132\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508351\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508316\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509138\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.50941\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.509299\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.509733\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.509961\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.51028\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508588\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6476435420204707, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6476435420204707\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008969452027075971, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008969452027075971\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9729628638536693e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9729628638536693e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9376879123977488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9376879123977488\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8561502879363431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8561502879363431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.02097450380407092, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02097450380407092\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006178813464274346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006178813464274346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8948539627930666, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8948539627930666\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8561502879363431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8561502879363431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.02097450380407092, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02097450380407092\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006178813464274346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006178813464274346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8948539627930666, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8948539627930666\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8561502879363431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8561502879363431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.02097450380407092, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02097450380407092\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006178813464274346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006178813464274346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8948539627930666, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8948539627930666\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508753\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508953\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509095\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.509087\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.50963\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509807\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.510325\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.510747\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.511299\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.511678\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.511793\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508753\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8561502879363431, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8561502879363431\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.02097450380407092, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02097450380407092\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006178813464274346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006178813464274346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8948539627930666, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8948539627930666\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6454902659121231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6454902659121231\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0006100959948175855, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006100959948175855\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.712653302658415e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.712653302658415e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6498628079763475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6498628079763475\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6454902659121231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6454902659121231\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0006100959948175855, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006100959948175855\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.712653302658415e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.712653302658415e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6498628079763475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6498628079763475\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6454902659121231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6454902659121231\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0006100959948175855, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006100959948175855\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.712653302658415e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.712653302658415e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6498628079763475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6498628079763475\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509061\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.509594\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.50965\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510792\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.510916\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.510824\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.510397\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510276\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.510513\tvalid_0's binary_error: 0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,514] Trial 3 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.11364070836973963, 'num_leaves': 96, 'max_depth': 10, 'min_data_in_leaf': 38, 'feature_fraction': 0.6454902659121231, 'bagging_fraction': 0.6498628079763475, 'bagging_freq': 4, 'lambda_l1': 0.0006100959948175855, 'lambda_l2': 6.712653302658415e-08}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,566] Trial 4 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.10862993126755824, 'num_leaves': 90, 'max_depth': 7, 'min_data_in_leaf': 17, 'feature_fraction': 0.5287726438178133, 'bagging_fraction': 0.5327834699951026, 'bagging_freq': 4, 'lambda_l1': 8.852543800186123e-08, 'lambda_l2': 0.00010384746312239315}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,608] Trial 5 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.03491191501521751, 'num_leaves': 15, 'max_depth': 9, 'min_data_in_leaf': 4, 'feature_fraction': 0.8445758319804253, 'bagging_fraction': 0.8765430801919343, 'bagging_freq': 4, 'lambda_l1': 3.9204170006498694e-08, 'lambda_l2': 4.452358414453458}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,652] Trial 6 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.14334968564962278, 'num_leaves': 33, 'max_depth': 10, 'min_data_in_leaf': 27, 'feature_fraction': 0.8460724404601723, 'bagging_fraction': 0.9232254876430817, 'bagging_freq': 7, 'lambda_l1': 3.937512672665447e-08, 'lambda_l2': 0.00016868057951713658}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,698] Trial 7 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.18890274510089575, 'num_leaves': 72, 'max_depth': 8, 'min_data_in_leaf': 41, 'feature_fraction': 0.7900986548661163, 'bagging_fraction': 0.6087251044927331, 'bagging_freq': 9, 'lambda_l1': 0.0005995606559908398, 'lambda_l2': 1.3696101496548893e-08}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.510325\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.510896\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509061\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=38\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6454902659121231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6454902659121231\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0006100959948175855, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006100959948175855\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.712653302658415e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.712653302658415e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6498628079763475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6498628079763475\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5287726438178133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5287726438178133\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.852543800186123e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.852543800186123e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010384746312239315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010384746312239315\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5327834699951026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5327834699951026\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5287726438178133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5287726438178133\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.852543800186123e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.852543800186123e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010384746312239315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010384746312239315\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5327834699951026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5327834699951026\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5287726438178133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5287726438178133\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.852543800186123e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.852543800186123e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010384746312239315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010384746312239315\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5327834699951026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5327834699951026\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509538\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.509934\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.5107\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.511604\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.512317\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.513269\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.513552\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.513898\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.514062\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.513401\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.513413\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509538\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5287726438178133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5287726438178133\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.852543800186123e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.852543800186123e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010384746312239315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010384746312239315\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5327834699951026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5327834699951026\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8445758319804253, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8445758319804253\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.9204170006498694e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.9204170006498694e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.452358414453458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.452358414453458\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8765430801919343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8765430801919343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8445758319804253, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8445758319804253\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.9204170006498694e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.9204170006498694e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.452358414453458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.452358414453458\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8765430801919343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8765430801919343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8445758319804253, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8445758319804253\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.9204170006498694e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.9204170006498694e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.452358414453458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.452358414453458\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8765430801919343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8765430801919343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508706\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508719\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.508756\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508799\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.508929\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509003\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509124\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.509368\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.509474\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.509458\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.509598\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508706\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8445758319804253, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8445758319804253\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.9204170006498694e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.9204170006498694e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.452358414453458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.452358414453458\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8765430801919343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8765430801919343\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8460724404601723, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8460724404601723\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.937512672665447e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.937512672665447e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00016868057951713658, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00016868057951713658\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9232254876430817, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9232254876430817\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8460724404601723, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8460724404601723\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.937512672665447e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.937512672665447e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00016868057951713658, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00016868057951713658\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9232254876430817, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9232254876430817\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8460724404601723, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8460724404601723\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.937512672665447e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.937512672665447e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00016868057951713658, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00016868057951713658\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9232254876430817, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9232254876430817\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508615\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508679\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.508702\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508982\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.508967\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509342\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509915\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.510206\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.510829\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.510784\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.510378\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508615\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8460724404601723, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8460724404601723\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.937512672665447e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.937512672665447e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00016868057951713658, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00016868057951713658\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9232254876430817, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9232254876430817\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=41, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=41\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7900986548661163, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7900986548661163\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005995606559908398, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005995606559908398\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3696101496548893e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3696101496548893e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6087251044927331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6087251044927331\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=41, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=41\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7900986548661163, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7900986548661163\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005995606559908398, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005995606559908398\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3696101496548893e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3696101496548893e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6087251044927331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6087251044927331\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=41, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=41\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7900986548661163, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7900986548661163\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005995606559908398, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005995606559908398\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3696101496548893e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3696101496548893e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6087251044927331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6087251044927331\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509024\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.509144\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509809\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.51154\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.512418\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.514062\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.515354\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.515634\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.515678\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.515772\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.516355\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509024\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=41, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=41\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7900986548661163, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7900986548661163\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005995606559908398, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005995606559908398\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3696101496548893e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3696101496548893e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6087251044927331, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6087251044927331\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,746] Trial 8 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.12350069022738384, 'num_leaves': 36, 'max_depth': 10, 'min_data_in_leaf': 33, 'feature_fraction': 0.986490607965683, 'bagging_fraction': 0.6018097489322238, 'bagging_freq': 5, 'lambda_l1': 2.2223775843731043e-08, 'lambda_l2': 6.300285233998752e-06}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,785] Trial 9 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.14844605897635837, 'num_leaves': 16, 'max_depth': 6, 'min_data_in_leaf': 7, 'feature_fraction': 0.6625423617069145, 'bagging_fraction': 0.8320523476340418, 'bagging_freq': 3, 'lambda_l1': 0.03338168815713874, 'lambda_l2': 0.3599006783303666}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:44,902] Trial 10 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.017049022895231872, 'num_leaves': 48, 'max_depth': 3, 'min_data_in_leaf': 49, 'feature_fraction': 0.9900499081215263, 'bagging_fraction': 0.766048809476699, 'bagging_freq': 1, 'lambda_l1': 2.526554165346011, 'lambda_l2': 0.011216482120856642}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986490607965683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986490607965683\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.2223775843731043e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2223775843731043e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.300285233998752e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.300285233998752e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6018097489322238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6018097489322238\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986490607965683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986490607965683\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.2223775843731043e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2223775843731043e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.300285233998752e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.300285233998752e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6018097489322238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6018097489322238\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986490607965683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986490607965683\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.2223775843731043e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2223775843731043e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.300285233998752e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.300285233998752e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6018097489322238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6018097489322238\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.509026\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.50925\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.510304\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.510681\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.510957\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.511027\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.510982\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.511764\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.512571\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.513246\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.512658\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509026\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986490607965683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986490607965683\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.2223775843731043e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2223775843731043e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.300285233998752e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.300285233998752e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6018097489322238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6018097489322238\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6625423617069145, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6625423617069145\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.03338168815713874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.03338168815713874\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3599006783303666, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3599006783303666\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8320523476340418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8320523476340418\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6625423617069145, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6625423617069145\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.03338168815713874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.03338168815713874\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3599006783303666, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3599006783303666\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8320523476340418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8320523476340418\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6625423617069145, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6625423617069145\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.03338168815713874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.03338168815713874\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3599006783303666, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3599006783303666\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8320523476340418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8320523476340418\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.509047\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.509396\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.509549\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.50968\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.510196\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.510814\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.51205\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.512441\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.512722\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.512731\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.512982\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509047\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6625423617069145, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6625423617069145\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.03338168815713874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.03338168815713874\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3599006783303666, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3599006783303666\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8320523476340418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8320523476340418\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=49, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=49\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9900499081215263, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9900499081215263\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.526554165346011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.526554165346011\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011216482120856642, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011216482120856642\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.766048809476699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.766048809476699\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=49, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=49\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9900499081215263, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9900499081215263\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.526554165346011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.526554165346011\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011216482120856642, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011216482120856642\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.766048809476699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.766048809476699\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=49, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=49\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9900499081215263, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9900499081215263\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.526554165346011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.526554165346011\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011216482120856642, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011216482120856642\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.766048809476699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.766048809476699\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508764\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508777\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508852\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508872\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508892\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.508908\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.508942\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.508976\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.508972\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.508992\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.508994\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508764\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=49, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=49\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9900499081215263, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9900499081215263\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.526554165346011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.526554165346011\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.011216482120856642, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011216482120856642\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.766048809476699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.766048809476699\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:45,033] Trial 11 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.2916611829365024, 'num_leaves': 66, 'max_depth': 5, 'min_data_in_leaf': 18, 'feature_fraction': 0.6832559344395834, 'bagging_fraction': 0.9886658307141019, 'bagging_freq': 7, 'lambda_l1': 3.183693515912166e-05, 'lambda_l2': 1.0249201326750515e-06}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:45,170] Trial 12 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.05520402538483891, 'num_leaves': 49, 'max_depth': 4, 'min_data_in_leaf': 18, 'feature_fraction': 0.5512616248290076, 'bagging_fraction': 0.990920637897916, 'bagging_freq': 1, 'lambda_l1': 9.513943319387515, 'lambda_l2': 0.023395077197993466}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6832559344395834, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6832559344395834\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.183693515912166e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.183693515912166e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0249201326750515e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0249201326750515e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9886658307141019, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9886658307141019\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6832559344395834, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6832559344395834\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.183693515912166e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.183693515912166e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0249201326750515e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0249201326750515e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9886658307141019, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9886658307141019\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6832559344395834, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6832559344395834\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.183693515912166e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.183693515912166e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0249201326750515e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0249201326750515e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9886658307141019, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9886658307141019\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509727\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.510794\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.512103\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.513644\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.514602\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.51551\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.516087\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.515723\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.516708\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.518711\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.519482\tvalid_0's binary_error: 0.2065\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509727\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6832559344395834, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6832559344395834\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.183693515912166e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.183693515912166e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0249201326750515e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0249201326750515e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9886658307141019, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9886658307141019\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5512616248290076, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512616248290076\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.513943319387515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.513943319387515\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.023395077197993466, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023395077197993466\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.990920637897916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.990920637897916\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5512616248290076, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512616248290076\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.513943319387515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.513943319387515\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.023395077197993466, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023395077197993466\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.990920637897916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.990920637897916\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5512616248290076, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512616248290076\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.513943319387515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.513943319387515\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.023395077197993466, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023395077197993466\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.990920637897916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.990920637897916\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508808\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508761\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508907\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.509065\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509134\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509255\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509221\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.509151\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.509146\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.50912\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509113\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508808\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5512616248290076, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512616248290076\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.513943319387515, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.513943319387515\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.023395077197993466, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023395077197993466\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.990920637897916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.990920637897916\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:45,305] Trial 13 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.01006379608040318, 'num_leaves': 85, 'max_depth': 6, 'min_data_in_leaf': 13, 'feature_fraction': 0.7329878381110371, 'bagging_fraction': 0.7622752691094659, 'bagging_freq': 10, 'lambda_l1': 5.13248558078144e-06, 'lambda_l2': 8.745534073688525}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:45,430] Trial 14 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.03158280140963705, 'num_leaves': 30, 'max_depth': 5, 'min_data_in_leaf': 22, 'feature_fraction': 0.5928773509516388, 'bagging_fraction': 0.8317422042044951, 'bagging_freq': 7, 'lambda_l1': 0.06652812078851476, 'lambda_l2': 0.017539637315232625}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7329878381110371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7329878381110371\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.13248558078144e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.13248558078144e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.745534073688525, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.745534073688525\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7622752691094659, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7622752691094659\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7329878381110371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7329878381110371\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.13248558078144e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.13248558078144e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.745534073688525, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.745534073688525\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7622752691094659, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7622752691094659\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7329878381110371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7329878381110371\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.13248558078144e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.13248558078144e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.745534073688525, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.745534073688525\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7622752691094659, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7622752691094659\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508753\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508758\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508803\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508847\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508865\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.508873\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.508894\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.508882\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.508894\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.508899\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.508892\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508753\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7329878381110371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7329878381110371\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.13248558078144e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.13248558078144e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=8.745534073688525, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.745534073688525\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7622752691094659, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7622752691094659\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5928773509516388, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5928773509516388\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06652812078851476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06652812078851476\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.017539637315232625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.017539637315232625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8317422042044951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8317422042044951\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5928773509516388, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5928773509516388\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06652812078851476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06652812078851476\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.017539637315232625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.017539637315232625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8317422042044951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8317422042044951\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5928773509516388, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5928773509516388\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06652812078851476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06652812078851476\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.017539637315232625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.017539637315232625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8317422042044951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8317422042044951\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508732\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508624\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508587\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508613\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508667\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.50871\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.508718\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.508599\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.508418\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.508325\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.508385\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508732\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5928773509516388, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5928773509516388\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06652812078851476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06652812078851476\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.017539637315232625, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.017539637315232625\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8317422042044951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8317422042044951\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 19:25:45,555] Trial 15 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.06953368930934174, 'num_leaves': 82, 'max_depth': 3, 'min_data_in_leaf': 11, 'feature_fraction': 0.7405071406600452, 'bagging_fraction': 0.942977522662068, 'bagging_freq': 8, 'lambda_l1': 0.4398455588203682, 'lambda_l2': 2.180160972965319e-06}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:45,692] Trial 16 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.28809549387527, 'num_leaves': 57, 'max_depth': 7, 'min_data_in_leaf': 24, 'feature_fraction': 0.5938227471526708, 'bagging_fraction': 0.8387937217355974, 'bagging_freq': 2, 'lambda_l1': 0.0028017757930905336, 'lambda_l2': 0.0017185900647161377}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7405071406600452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7405071406600452\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4398455588203682, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4398455588203682\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.180160972965319e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.180160972965319e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.942977522662068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.942977522662068\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7405071406600452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7405071406600452\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4398455588203682, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4398455588203682\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.180160972965319e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.180160972965319e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.942977522662068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.942977522662068\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7405071406600452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7405071406600452\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4398455588203682, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4398455588203682\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.180160972965319e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.180160972965319e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.942977522662068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.942977522662068\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.50868\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508607\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508811\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508806\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509027\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509024\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509179\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.509174\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.509393\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.509431\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509566\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.50868\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7405071406600452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7405071406600452\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4398455588203682, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4398455588203682\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.180160972965319e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.180160972965319e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.942977522662068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.942977522662068\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=24, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=24\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5938227471526708, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5938227471526708\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0028017757930905336, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0028017757930905336\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0017185900647161377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017185900647161377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387937217355974, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8387937217355974\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=24, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=24\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5938227471526708, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5938227471526708\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0028017757930905336, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0028017757930905336\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0017185900647161377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017185900647161377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387937217355974, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8387937217355974\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=24, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=24\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5938227471526708, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5938227471526708\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0028017757930905336, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0028017757930905336\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0017185900647161377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017185900647161377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387937217355974, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8387937217355974\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.511124\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.510437\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.51243\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.514491\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.516613\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.516554\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.519111\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.519588\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.519881\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.519463\tvalid_0's binary_error: 0.2065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.520049\tvalid_0's binary_error: 0.207\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.511124\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=24, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=24\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5938227471526708, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5938227471526708\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0028017757930905336, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0028017757930905336\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0017185900647161377, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017185900647161377\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387937217355974, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8387937217355974\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 19:25:45,823] Trial 17 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.07773033973817091, 'num_leaves': 24, 'max_depth': 5, 'min_data_in_leaf': 34, 'feature_fraction': 0.7898181499390814, 'bagging_fraction': 0.6957204877333496, 'bagging_freq': 6, 'lambda_l1': 1.814386169902896e-05, 'lambda_l2': 0.11522668777807676}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:45,978] Trial 18 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.032458159747504066, 'num_leaves': 100, 'max_depth': 8, 'min_data_in_leaf': 23, 'feature_fraction': 0.9234137893422333, 'bagging_fraction': 0.9429794292845246, 'bagging_freq': 6, 'lambda_l1': 0.005779643534927643, 'lambda_l2': 5.471962108537456e-05}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7898181499390814, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7898181499390814\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.814386169902896e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.814386169902896e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.11522668777807676, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.11522668777807676\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6957204877333496, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6957204877333496\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7898181499390814, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7898181499390814\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.814386169902896e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.814386169902896e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.11522668777807676, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.11522668777807676\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6957204877333496, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6957204877333496\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7898181499390814, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7898181499390814\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.814386169902896e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.814386169902896e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.11522668777807676, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.11522668777807676\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6957204877333496, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6957204877333496\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508966\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508838\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.509163\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.509536\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509511\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509481\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509644\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510012\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.5104\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.510196\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.510375\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508966\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7898181499390814, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7898181499390814\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.814386169902896e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.814386169902896e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.11522668777807676, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.11522668777807676\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6957204877333496, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6957204877333496\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9234137893422333, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9234137893422333\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.005779643534927643, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005779643534927643\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.471962108537456e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.471962108537456e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9429794292845246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9429794292845246\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9234137893422333, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9234137893422333\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.005779643534927643, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005779643534927643\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.471962108537456e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.471962108537456e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9429794292845246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9429794292845246\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9234137893422333, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9234137893422333\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.005779643534927643, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005779643534927643\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.471962108537456e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.471962108537456e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9429794292845246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9429794292845246\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508713\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508708\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508827\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508757\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508911\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.508884\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509019\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.509155\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.509403\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.509514\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509691\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508713\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9234137893422333, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9234137893422333\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.005779643534927643, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.005779643534927643\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.471962108537456e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.471962108537456e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9429794292845246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9429794292845246\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,107] Trial 19 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.21025306338973482, 'num_leaves': 42, 'max_depth': 4, 'min_data_in_leaf': 44, 'feature_fraction': 0.7080649289235946, 'bagging_fraction': 0.9826241662199484, 'bagging_freq': 2, 'lambda_l1': 0.2905555248621703, 'lambda_l2': 1.93587179613063e-07}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,245] Trial 20 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.019841962495661412, 'num_leaves': 61, 'max_depth': 9, 'min_data_in_leaf': 11, 'feature_fraction': 0.6250159920435079, 'bagging_fraction': 0.7917786755874107, 'bagging_freq': 9, 'lambda_l1': 6.701765337693026e-07, 'lambda_l2': 0.415946524874369}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7080649289235946, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7080649289235946\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2905555248621703, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2905555248621703\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.93587179613063e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.93587179613063e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9826241662199484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9826241662199484\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7080649289235946, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7080649289235946\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2905555248621703, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2905555248621703\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.93587179613063e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.93587179613063e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9826241662199484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9826241662199484\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7080649289235946, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7080649289235946\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2905555248621703, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2905555248621703\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.93587179613063e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.93587179613063e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9826241662199484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9826241662199484\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508823\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508971\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.510002\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510797\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.511269\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.512116\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.512532\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.512651\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.513206\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.51337\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.513948\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508823\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7080649289235946, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7080649289235946\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.2905555248621703, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2905555248621703\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.93587179613063e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.93587179613063e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9826241662199484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9826241662199484\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6250159920435079, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6250159920435079\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.701765337693026e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.701765337693026e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.415946524874369, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.415946524874369\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7917786755874107, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7917786755874107\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6250159920435079, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6250159920435079\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.701765337693026e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.701765337693026e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.415946524874369, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.415946524874369\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7917786755874107, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7917786755874107\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6250159920435079, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6250159920435079\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.701765337693026e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.701765337693026e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.415946524874369, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.415946524874369\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7917786755874107, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7917786755874107\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508805\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508768\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.508885\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508999\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.509045\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509059\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.508976\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.509075\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.508942\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.508975\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.50894\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508805\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6250159920435079, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6250159920435079\n",
      "[LightGBM] [Warning] lambda_l1 is set=6.701765337693026e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.701765337693026e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.415946524874369, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.415946524874369\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7917786755874107, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7917786755874107\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,394] Trial 21 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.04236185587997001, 'num_leaves': 74, 'max_depth': 8, 'min_data_in_leaf': 29, 'feature_fraction': 0.8579975658955403, 'bagging_fraction': 0.8999003264998406, 'bagging_freq': 3, 'lambda_l1': 0.008992095932068423, 'lambda_l2': 0.0036170596587033316}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,543] Trial 22 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.07921031313999535, 'num_leaves': 80, 'max_depth': 9, 'min_data_in_leaf': 31, 'feature_fraction': 0.7902280160338911, 'bagging_fraction': 0.8883843506034811, 'bagging_freq': 5, 'lambda_l1': 8.047015364379566e-05, 'lambda_l2': 2.082352845235809e-05}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8579975658955403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8579975658955403\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008992095932068423, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008992095932068423\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0036170596587033316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0036170596587033316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8999003264998406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8999003264998406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8579975658955403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8579975658955403\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008992095932068423, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008992095932068423\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0036170596587033316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0036170596587033316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8999003264998406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8999003264998406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8579975658955403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8579975658955403\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008992095932068423, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008992095932068423\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0036170596587033316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0036170596587033316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8999003264998406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8999003264998406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508762\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508707\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508817\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508809\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.509291\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509772\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509723\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.510019\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.510213\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.510218\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.510475\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508762\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=29, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8579975658955403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8579975658955403\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.008992095932068423, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.008992095932068423\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0036170596587033316, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0036170596587033316\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8999003264998406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8999003264998406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902280160338911, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902280160338911\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.047015364379566e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.047015364379566e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.082352845235809e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.082352845235809e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883843506034811, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883843506034811\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902280160338911, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902280160338911\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.047015364379566e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.047015364379566e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.082352845235809e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.082352845235809e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883843506034811, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883843506034811\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902280160338911, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902280160338911\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.047015364379566e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.047015364379566e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.082352845235809e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.082352845235809e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883843506034811, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883843506034811\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.50891\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.509075\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.50973\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510342\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.510457\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.511175\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.51155\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.512785\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.513473\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.513889\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.513986\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.50891\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7902280160338911, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902280160338911\n",
      "[LightGBM] [Warning] lambda_l1 is set=8.047015364379566e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.047015364379566e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.082352845235809e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.082352845235809e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883843506034811, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883843506034811\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,680] Trial 23 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.05222325615700999, 'num_leaves': 66, 'max_depth': 7, 'min_data_in_leaf': 36, 'feature_fraction': 0.8974068712095608, 'bagging_fraction': 0.938683127279587, 'bagging_freq': 3, 'lambda_l1': 0.04167871239023017, 'lambda_l2': 0.0006509864098250989}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,813] Trial 24 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.020683569604305094, 'num_leaves': 90, 'max_depth': 9, 'min_data_in_leaf': 27, 'feature_fraction': 0.7869488937707596, 'bagging_fraction': 0.8518695831336335, 'bagging_freq': 2, 'lambda_l1': 0.0023176637311944095, 'lambda_l2': 0.0005118527203895488}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8974068712095608, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8974068712095608\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04167871239023017, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04167871239023017\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006509864098250989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006509864098250989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.938683127279587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.938683127279587\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8974068712095608, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8974068712095608\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04167871239023017, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04167871239023017\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006509864098250989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006509864098250989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.938683127279587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.938683127279587\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8974068712095608, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8974068712095608\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04167871239023017, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04167871239023017\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006509864098250989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006509864098250989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.938683127279587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.938683127279587\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508774\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508752\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508951\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.509076\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509497\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509775\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.510199\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510362\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.510671\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.510873\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.511191\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508774\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8974068712095608, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8974068712095608\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04167871239023017, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04167871239023017\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0006509864098250989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006509864098250989\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.938683127279587, subsample=1.0 will be ignored. Current value: bagging_fraction=0.938683127279587\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7869488937707596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7869488937707596\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0023176637311944095, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0023176637311944095\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005118527203895488, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005118527203895488\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518695831336335, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518695831336335\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7869488937707596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7869488937707596\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0023176637311944095, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0023176637311944095\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005118527203895488, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005118527203895488\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518695831336335, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518695831336335\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7869488937707596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7869488937707596\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0023176637311944095, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0023176637311944095\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005118527203895488, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005118527203895488\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518695831336335, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518695831336335\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508788\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.50889\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.508966\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508969\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508966\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.508988\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509157\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.509302\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.509349\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.509292\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509379\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508788\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7869488937707596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7869488937707596\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0023176637311944095, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0023176637311944095\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0005118527203895488, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005118527203895488\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8518695831336335, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8518695831336335\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:46,947] Trial 25 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.09287189182845558, 'num_leaves': 22, 'max_depth': 6, 'min_data_in_leaf': 21, 'feature_fraction': 0.9127524670668095, 'bagging_fraction': 0.9636007819542838, 'bagging_freq': 5, 'lambda_l1': 0.49455315880307155, 'lambda_l2': 1.4032559546238725}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,072] Trial 26 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.056286087953867005, 'num_leaves': 10, 'max_depth': 8, 'min_data_in_leaf': 15, 'feature_fraction': 0.7020637180633144, 'bagging_fraction': 0.712998593260949, 'bagging_freq': 6, 'lambda_l1': 0.00024840227506422546, 'lambda_l2': 0.08017558111229889}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9127524670668095, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9127524670668095\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49455315880307155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49455315880307155\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.4032559546238725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4032559546238725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9636007819542838, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9636007819542838\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9127524670668095, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9127524670668095\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49455315880307155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49455315880307155\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.4032559546238725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4032559546238725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9636007819542838, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9636007819542838\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9127524670668095, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9127524670668095\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49455315880307155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49455315880307155\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.4032559546238725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4032559546238725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9636007819542838, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9636007819542838\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508934\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.509204\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.509475\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.509841\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.510109\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.510606\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.510969\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.511363\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.511719\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.511785\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.511878\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508934\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=21, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=21\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9127524670668095, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9127524670668095\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.49455315880307155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.49455315880307155\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.4032559546238725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4032559546238725\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9636007819542838, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9636007819542838\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7020637180633144, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7020637180633144\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00024840227506422546, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024840227506422546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08017558111229889, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08017558111229889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.712998593260949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.712998593260949\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7020637180633144, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7020637180633144\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00024840227506422546, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024840227506422546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08017558111229889, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08017558111229889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.712998593260949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.712998593260949\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7020637180633144, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7020637180633144\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00024840227506422546, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024840227506422546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08017558111229889, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08017558111229889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.712998593260949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.712998593260949\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508752\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508748\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.508851\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.50891\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.508993\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.509175\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509239\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.509228\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.509425\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.509207\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.509259\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508752\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7020637180633144, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7020637180633144\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00024840227506422546, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024840227506422546\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.08017558111229889, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.08017558111229889\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.712998593260949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.712998593260949\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,208] Trial 27 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.20607268644476287, 'num_leaves': 51, 'max_depth': 10, 'min_data_in_leaf': 30, 'feature_fraction': 0.7573882363180039, 'bagging_fraction': 0.9044405475435222, 'bagging_freq': 8, 'lambda_l1': 0.1176160142899211, 'lambda_l2': 0.004240337297249717}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,344] Trial 28 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.024458684680123317, 'num_leaves': 44, 'max_depth': 7, 'min_data_in_leaf': 26, 'feature_fraction': 0.825262156221884, 'bagging_fraction': 0.8070888344011983, 'bagging_freq': 4, 'lambda_l1': 0.012882690095250702, 'lambda_l2': 9.011552672638978e-06}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7573882363180039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7573882363180039\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1176160142899211, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1176160142899211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.004240337297249717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004240337297249717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9044405475435222, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9044405475435222\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7573882363180039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7573882363180039\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1176160142899211, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1176160142899211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.004240337297249717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004240337297249717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9044405475435222, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9044405475435222\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7573882363180039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7573882363180039\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1176160142899211, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1176160142899211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.004240337297249717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004240337297249717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9044405475435222, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9044405475435222\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.509521\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.509639\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.512056\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.513731\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.514234\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.515367\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.516489\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.517185\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.51856\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.519004\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.520306\tvalid_0's binary_error: 0.2065\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509521\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7573882363180039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7573882363180039\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1176160142899211, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1176160142899211\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.004240337297249717, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004240337297249717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9044405475435222, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9044405475435222\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.825262156221884, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.825262156221884\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.012882690095250702, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012882690095250702\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.011552672638978e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.011552672638978e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8070888344011983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8070888344011983\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.825262156221884, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.825262156221884\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.012882690095250702, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012882690095250702\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.011552672638978e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.011552672638978e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8070888344011983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8070888344011983\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.825262156221884, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.825262156221884\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.012882690095250702, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012882690095250702\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.011552672638978e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.011552672638978e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8070888344011983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8070888344011983\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.5087\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508624\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508702\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508786\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508913\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.508975\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.509043\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.509079\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.509168\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.509234\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.509363\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.5087\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n",
      "[LightGBM] [Warning] feature_fraction is set=0.825262156221884, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.825262156221884\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.012882690095250702, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012882690095250702\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.011552672638978e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.011552672638978e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8070888344011983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8070888344011983\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,469] Trial 29 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.012595075607684641, 'num_leaves': 72, 'max_depth': 4, 'min_data_in_leaf': 37, 'feature_fraction': 0.6541097964230445, 'bagging_fraction': 0.8819050930712954, 'bagging_freq': 4, 'lambda_l1': 0.0008559174457693692, 'lambda_l2': 1.577472470088125e-07}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,619] Trial 30 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.04177475026006605, 'num_leaves': 93, 'max_depth': 10, 'min_data_in_leaf': 20, 'feature_fraction': 0.6171891156951435, 'bagging_fraction': 0.9571666756165385, 'bagging_freq': 3, 'lambda_l1': 1.5978108106657294, 'lambda_l2': 1.208203228165522e-08}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=37\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6541097964230445, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6541097964230445\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008559174457693692, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008559174457693692\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.577472470088125e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.577472470088125e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8819050930712954, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8819050930712954\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=37\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6541097964230445, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6541097964230445\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008559174457693692, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008559174457693692\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.577472470088125e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.577472470088125e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8819050930712954, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8819050930712954\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=37\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6541097964230445, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6541097964230445\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008559174457693692, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008559174457693692\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.577472470088125e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.577472470088125e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8819050930712954, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8819050930712954\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508684\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508681\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508654\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.50863\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.508688\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.508732\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.508788\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.508822\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.508823\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.508804\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.508804\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508684\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=37\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6541097964230445, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6541097964230445\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0008559174457693692, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0008559174457693692\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.577472470088125e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.577472470088125e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8819050930712954, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8819050930712954\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6171891156951435, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6171891156951435\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5978108106657294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5978108106657294\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.208203228165522e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.208203228165522e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9571666756165385, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9571666756165385\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6171891156951435, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6171891156951435\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5978108106657294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5978108106657294\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.208203228165522e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.208203228165522e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9571666756165385, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9571666756165385\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6171891156951435, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6171891156951435\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5978108106657294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5978108106657294\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.208203228165522e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.208203228165522e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9571666756165385, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9571666756165385\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508845\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508835\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.509017\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.509343\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.509693\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.5101\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.510393\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.510799\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.51059\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.510545\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.510557\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508845\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6171891156951435, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6171891156951435\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.5978108106657294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5978108106657294\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.208203228165522e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.208203228165522e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9571666756165385, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9571666756165385\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,760] Trial 31 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.10529401401783102, 'num_leaves': 99, 'max_depth': 9, 'min_data_in_leaf': 42, 'feature_fraction': 0.5401301129916181, 'bagging_fraction': 0.6109736418520115, 'bagging_freq': 5, 'lambda_l1': 0.0002609508883040656, 'lambda_l2': 9.873953587128586e-08}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:47,899] Trial 32 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.1608869133401516, 'num_leaves': 95, 'max_depth': 10, 'min_data_in_leaf': 36, 'feature_fraction': 0.5030254233147136, 'bagging_fraction': 0.6554760659525661, 'bagging_freq': 4, 'lambda_l1': 0.0017177847616344126, 'lambda_l2': 6.428275022482273e-08}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=42\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5401301129916181, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5401301129916181\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0002609508883040656, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002609508883040656\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.873953587128586e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.873953587128586e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6109736418520115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6109736418520115\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=42\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5401301129916181, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5401301129916181\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0002609508883040656, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002609508883040656\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.873953587128586e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.873953587128586e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6109736418520115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6109736418520115\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=42\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5401301129916181, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5401301129916181\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0002609508883040656, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002609508883040656\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.873953587128586e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.873953587128586e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6109736418520115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6109736418520115\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508846\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508919\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509695\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.50997\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.510294\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.510356\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.510253\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.51015\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.509988\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.510012\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509081\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508846\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=42\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5401301129916181, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5401301129916181\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0002609508883040656, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002609508883040656\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.873953587128586e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.873953587128586e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6109736418520115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6109736418520115\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5030254233147136, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5030254233147136\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017177847616344126, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017177847616344126\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.428275022482273e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.428275022482273e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6554760659525661, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6554760659525661\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5030254233147136, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5030254233147136\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017177847616344126, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017177847616344126\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.428275022482273e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.428275022482273e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6554760659525661, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6554760659525661\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5030254233147136, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5030254233147136\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017177847616344126, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017177847616344126\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.428275022482273e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.428275022482273e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6554760659525661, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6554760659525661\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509219\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.50857\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.50891\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510555\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.510709\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.513051\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.513522\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.514254\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.513733\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.513555\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.51418\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509219\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=36, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=36\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5030254233147136, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5030254233147136\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0017177847616344126, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0017177847616344126\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.428275022482273e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.428275022482273e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6554760659525661, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6554760659525661\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,036] Trial 33 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.12004355273757973, 'num_leaves': 86, 'max_depth': 9, 'min_data_in_leaf': 40, 'feature_fraction': 0.6439047707352363, 'bagging_fraction': 0.5174048654405956, 'bagging_freq': 2, 'lambda_l1': 0.013928143257528782, 'lambda_l2': 6.113127897641808e-07}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,169] Trial 34 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.09443961177488497, 'num_leaves': 77, 'max_depth': 10, 'min_data_in_leaf': 48, 'feature_fraction': 0.7110142675370943, 'bagging_fraction': 0.5567009173790558, 'bagging_freq': 4, 'lambda_l1': 0.00013367747494702874, 'lambda_l2': 2.740883224183735e-08}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6439047707352363, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6439047707352363\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.013928143257528782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013928143257528782\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.113127897641808e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.113127897641808e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5174048654405956, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5174048654405956\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6439047707352363, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6439047707352363\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.013928143257528782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013928143257528782\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.113127897641808e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.113127897641808e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5174048654405956, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5174048654405956\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6439047707352363, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6439047707352363\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.013928143257528782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013928143257528782\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.113127897641808e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.113127897641808e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5174048654405956, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5174048654405956\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509684\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.509832\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.510714\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.511148\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.51165\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.511706\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.512021\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.512086\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.512274\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.513161\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.514147\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509684\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6439047707352363, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6439047707352363\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.013928143257528782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.013928143257528782\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.113127897641808e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.113127897641808e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5174048654405956, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5174048654405956\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=48, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=48\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7110142675370943, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7110142675370943\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00013367747494702874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00013367747494702874\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.740883224183735e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.740883224183735e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5567009173790558, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5567009173790558\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=48, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=48\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7110142675370943, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7110142675370943\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00013367747494702874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00013367747494702874\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.740883224183735e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.740883224183735e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5567009173790558, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5567009173790558\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=48, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=48\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7110142675370943, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7110142675370943\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00013367747494702874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00013367747494702874\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.740883224183735e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.740883224183735e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5567009173790558, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5567009173790558\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.509124\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508548\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509673\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510586\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.510484\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.511521\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.511636\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.512711\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.512712\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.512455\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.512793\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509124\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=48, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=48\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7110142675370943, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7110142675370943\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.00013367747494702874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00013367747494702874\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.740883224183735e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.740883224183735e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5567009173790558, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5567009173790558\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,301] Trial 35 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.06579842955357602, 'num_leaves': 89, 'max_depth': 8, 'min_data_in_leaf': 32, 'feature_fraction': 0.6810054047983765, 'bagging_fraction': 0.713417847936991, 'bagging_freq': 8, 'lambda_l1': 0.0011546616877342098, 'lambda_l2': 0.00010013169785182057}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,451] Trial 36 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.1717878450384521, 'num_leaves': 95, 'max_depth': 10, 'min_data_in_leaf': 4, 'feature_fraction': 0.5910869690269747, 'bagging_fraction': 0.6565385886333266, 'bagging_freq': 1, 'lambda_l1': 3.624032363816998e-06, 'lambda_l2': 6.989574288467825e-07}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6810054047983765, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6810054047983765\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0011546616877342098, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0011546616877342098\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010013169785182057, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010013169785182057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.713417847936991, subsample=1.0 will be ignored. Current value: bagging_fraction=0.713417847936991\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6810054047983765, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6810054047983765\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0011546616877342098, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0011546616877342098\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010013169785182057, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010013169785182057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.713417847936991, subsample=1.0 will be ignored. Current value: bagging_fraction=0.713417847936991\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6810054047983765, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6810054047983765\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0011546616877342098, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0011546616877342098\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010013169785182057, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010013169785182057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.713417847936991, subsample=1.0 will be ignored. Current value: bagging_fraction=0.713417847936991\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508817\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508602\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508544\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.509108\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509477\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509634\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.510141\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510918\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.51177\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.511588\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.511251\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508817\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=32\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6810054047983765, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6810054047983765\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0011546616877342098, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0011546616877342098\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00010013169785182057, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010013169785182057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.713417847936991, subsample=1.0 will be ignored. Current value: bagging_fraction=0.713417847936991\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5910869690269747, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5910869690269747\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.624032363816998e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.624032363816998e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.989574288467825e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989574288467825e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6565385886333266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6565385886333266\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5910869690269747, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5910869690269747\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.624032363816998e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.624032363816998e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.989574288467825e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989574288467825e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6565385886333266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6565385886333266\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5910869690269747, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5910869690269747\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.624032363816998e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.624032363816998e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.989574288467825e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989574288467825e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6565385886333266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6565385886333266\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.512259\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.511827\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.513159\tvalid_0's binary_error: 0.207\n",
      "[4]\tvalid_0's binary_logloss: 0.515433\tvalid_0's binary_error: 0.2065\n",
      "[5]\tvalid_0's binary_logloss: 0.516405\tvalid_0's binary_error: 0.207\n",
      "[6]\tvalid_0's binary_logloss: 0.517439\tvalid_0's binary_error: 0.207\n",
      "[7]\tvalid_0's binary_logloss: 0.517681\tvalid_0's binary_error: 0.207\n",
      "[8]\tvalid_0's binary_logloss: 0.517299\tvalid_0's binary_error: 0.208\n",
      "[9]\tvalid_0's binary_logloss: 0.517163\tvalid_0's binary_error: 0.209\n",
      "[10]\tvalid_0's binary_logloss: 0.517321\tvalid_0's binary_error: 0.2085\n",
      "[11]\tvalid_0's binary_logloss: 0.5182\tvalid_0's binary_error: 0.209\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.512259\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5910869690269747, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5910869690269747\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.624032363816998e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.624032363816998e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.989574288467825e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989574288467825e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6565385886333266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6565385886333266\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,592] Trial 37 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.1333281539397612, 'num_leaves': 65, 'max_depth': 9, 'min_data_in_leaf': 28, 'feature_fraction': 0.9558872990908697, 'bagging_fraction': 0.8640088925985906, 'bagging_freq': 3, 'lambda_l1': 0.0005735150294288353, 'lambda_l2': 2.880041324801619e-06}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,711] Trial 38 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.2379989391821248, 'num_leaves': 38, 'max_depth': 6, 'min_data_in_leaf': 40, 'feature_fraction': 0.8655117811630545, 'bagging_fraction': 0.5722916505368314, 'bagging_freq': 5, 'lambda_l1': 0.023092168664236395, 'lambda_l2': 3.6515870781701275e-08}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=28, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9558872990908697, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9558872990908697\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005735150294288353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005735150294288353\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.880041324801619e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.880041324801619e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8640088925985906, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8640088925985906\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=28, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9558872990908697, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9558872990908697\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005735150294288353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005735150294288353\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.880041324801619e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.880041324801619e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8640088925985906, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8640088925985906\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=28, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9558872990908697, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9558872990908697\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005735150294288353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005735150294288353\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.880041324801619e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.880041324801619e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8640088925985906, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8640088925985906\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.509229\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.509832\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.509967\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.510269\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.511481\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.512757\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.513187\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.514363\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.516946\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.516648\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.517619\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509229\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=28, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9558872990908697, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9558872990908697\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0005735150294288353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005735150294288353\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.880041324801619e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.880041324801619e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8640088925985906, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8640088925985906\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8655117811630545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8655117811630545\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.023092168664236395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.023092168664236395\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.6515870781701275e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.6515870781701275e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5722916505368314, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5722916505368314\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8655117811630545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8655117811630545\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.023092168664236395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.023092168664236395\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.6515870781701275e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.6515870781701275e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5722916505368314, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5722916505368314\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8655117811630545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8655117811630545\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.023092168664236395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.023092168664236395\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.6515870781701275e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.6515870781701275e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5722916505368314, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5722916505368314\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.51099\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.512021\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.511934\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.514319\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.5146\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.51422\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.515301\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.517032\tvalid_0's binary_error: 0.207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.517798\tvalid_0's binary_error: 0.207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.518337\tvalid_0's binary_error: 0.207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.519421\tvalid_0's binary_error: 0.207\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.51099\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8655117811630545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8655117811630545\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.023092168664236395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.023092168664236395\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.6515870781701275e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.6515870781701275e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5722916505368314, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5722916505368314\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,846] Trial 39 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.04554798631422862, 'num_leaves': 26, 'max_depth': 8, 'min_data_in_leaf': 25, 'feature_fraction': 0.8206555493012211, 'bagging_fraction': 0.9259523572882217, 'bagging_freq': 7, 'lambda_l1': 0.10999680529348291, 'lambda_l2': 1.3378419590256453e-05}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:48,984] Trial 40 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.11141467815840857, 'num_leaves': 100, 'max_depth': 7, 'min_data_in_leaf': 46, 'feature_fraction': 0.5630203677018982, 'bagging_fraction': 0.9981238701504069, 'bagging_freq': 4, 'lambda_l1': 0.0038214538843489134, 'lambda_l2': 2.7748771061190483}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8206555493012211, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8206555493012211\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.10999680529348291, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.10999680529348291\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3378419590256453e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3378419590256453e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9259523572882217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259523572882217\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8206555493012211, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8206555493012211\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.10999680529348291, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.10999680529348291\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3378419590256453e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3378419590256453e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9259523572882217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259523572882217\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8206555493012211, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8206555493012211\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.10999680529348291, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.10999680529348291\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3378419590256453e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3378419590256453e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9259523572882217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259523572882217\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.508593\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.508584\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.50849\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.508497\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.508827\tvalid_0's binary_error: 0.206\n",
      "[6]\tvalid_0's binary_logloss: 0.508888\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.508996\tvalid_0's binary_error: 0.206\n",
      "[8]\tvalid_0's binary_logloss: 0.50932\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.50964\tvalid_0's binary_error: 0.206\n",
      "[10]\tvalid_0's binary_logloss: 0.509631\tvalid_0's binary_error: 0.206\n",
      "[11]\tvalid_0's binary_logloss: 0.509938\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508593\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8206555493012211, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8206555493012211\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.10999680529348291, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.10999680529348291\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3378419590256453e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3378419590256453e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9259523572882217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9259523572882217\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5630203677018982, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5630203677018982\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0038214538843489134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0038214538843489134\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.7748771061190483, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7748771061190483\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9981238701504069, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9981238701504069\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5630203677018982, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5630203677018982\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0038214538843489134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0038214538843489134\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.7748771061190483, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7748771061190483\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9981238701504069, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9981238701504069\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5630203677018982, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5630203677018982\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0038214538843489134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0038214538843489134\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.7748771061190483, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7748771061190483\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9981238701504069, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9981238701504069\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508712\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.50834\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508771\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508921\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509285\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509951\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.510099\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510214\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.510409\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.509995\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.510072\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508712\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=46, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=46\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5630203677018982, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5630203677018982\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0038214538843489134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0038214538843489134\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.7748771061190483, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7748771061190483\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9981238701504069, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9981238701504069\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,125] Trial 41 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.13795338996057988, 'num_leaves': 90, 'max_depth': 6, 'min_data_in_leaf': 18, 'feature_fraction': 0.528124318417206, 'bagging_fraction': 0.6612826991000937, 'bagging_freq': 6, 'lambda_l1': 1.3870923359710007e-07, 'lambda_l2': 0.00022596669709006763}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,248] Trial 42 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.08849996235342762, 'num_leaves': 84, 'max_depth': 5, 'min_data_in_leaf': 14, 'feature_fraction': 0.6725836816703278, 'bagging_fraction': 0.5852496861231805, 'bagging_freq': 3, 'lambda_l1': 1.0026659461230703e-07, 'lambda_l2': 3.571479948395143e-07}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.528124318417206, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.528124318417206\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.3870923359710007e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3870923359710007e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00022596669709006763, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00022596669709006763\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6612826991000937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6612826991000937\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.528124318417206, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.528124318417206\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.3870923359710007e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3870923359710007e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00022596669709006763, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00022596669709006763\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6612826991000937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6612826991000937\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.528124318417206, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.528124318417206\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.3870923359710007e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3870923359710007e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00022596669709006763, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00022596669709006763\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6612826991000937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6612826991000937\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.5095\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508697\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508505\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508951\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509029\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509331\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509379\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.509062\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.508485\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.509188\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.50888\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.5095\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=18, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=18\n",
      "[LightGBM] [Warning] feature_fraction is set=0.528124318417206, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.528124318417206\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.3870923359710007e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3870923359710007e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.00022596669709006763, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00022596669709006763\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6612826991000937, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6612826991000937\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6725836816703278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6725836816703278\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0026659461230703e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0026659461230703e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.571479948395143e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.571479948395143e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5852496861231805, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5852496861231805\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6725836816703278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6725836816703278\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0026659461230703e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0026659461230703e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.571479948395143e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.571479948395143e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5852496861231805, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5852496861231805\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6725836816703278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6725836816703278\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0026659461230703e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0026659461230703e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.571479948395143e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.571479948395143e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5852496861231805, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5852496861231805\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508784\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.50908\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509597\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.509541\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509515\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509642\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509887\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510211\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.510158\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.510533\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.510894\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508784\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6725836816703278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6725836816703278\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0026659461230703e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0026659461230703e-07\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.571479948395143e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.571479948395143e-07\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5852496861231805, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5852496861231805\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,390] Trial 43 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.06510907888998727, 'num_leaves': 94, 'max_depth': 7, 'min_data_in_leaf': 8, 'feature_fraction': 0.7632544788963732, 'bagging_fraction': 0.5447191833551336, 'bagging_freq': 5, 'lambda_l1': 1.0785766141479555e-08, 'lambda_l2': 1.0621603298739431e-08}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,540] Trial 44 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.1742964366033143, 'num_leaves': 80, 'max_depth': 10, 'min_data_in_leaf': 16, 'feature_fraction': 0.5039914300342505, 'bagging_fraction': 0.6235344712853501, 'bagging_freq': 2, 'lambda_l1': 3.223114340902029e-05, 'lambda_l2': 0.079872827005918}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7632544788963732, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7632544788963732\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0785766141479555e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0785766141479555e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0621603298739431e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0621603298739431e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447191833551336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5447191833551336\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7632544788963732, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7632544788963732\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0785766141479555e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0785766141479555e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0621603298739431e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0621603298739431e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447191833551336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5447191833551336\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7632544788963732, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7632544788963732\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0785766141479555e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0785766141479555e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0621603298739431e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0621603298739431e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447191833551336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5447191833551336\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.5089\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508896\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509661\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510225\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509828\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.51003\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.51064\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.510608\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.510294\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.510155\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509963\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.5089\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7632544788963732, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7632544788963732\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0785766141479555e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0785766141479555e-08\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0621603298739431e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0621603298739431e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5447191833551336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5447191833551336\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5039914300342505, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5039914300342505\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.223114340902029e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.223114340902029e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.079872827005918, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.079872827005918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6235344712853501, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6235344712853501\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5039914300342505, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5039914300342505\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.223114340902029e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.223114340902029e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.079872827005918, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.079872827005918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6235344712853501, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6235344712853501\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5039914300342505, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5039914300342505\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.223114340902029e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.223114340902029e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.079872827005918, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.079872827005918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6235344712853501, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6235344712853501\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[1]\tvalid_0's binary_logloss: 0.510926\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.510421\tvalid_0's binary_error: 0.206\n",
      "[3]\tvalid_0's binary_logloss: 0.511968\tvalid_0's binary_error: 0.206\n",
      "[4]\tvalid_0's binary_logloss: 0.514765\tvalid_0's binary_error: 0.206\n",
      "[5]\tvalid_0's binary_logloss: 0.516303\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.516263\tvalid_0's binary_error: 0.206\n",
      "[7]\tvalid_0's binary_logloss: 0.517578\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.519226\tvalid_0's binary_error: 0.206\n",
      "[9]\tvalid_0's binary_logloss: 0.52006\tvalid_0's binary_error: 0.2065\n",
      "[10]\tvalid_0's binary_logloss: 0.521039\tvalid_0's binary_error: 0.207\n",
      "[11]\tvalid_0's binary_logloss: 0.522034\tvalid_0's binary_error: 0.207\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.510926\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=16, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5039914300342505, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5039914300342505\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.223114340902029e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.223114340902029e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.079872827005918, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.079872827005918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6235344712853501, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6235344712853501\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,677] Trial 45 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.1461250078908495, 'num_leaves': 54, 'max_depth': 5, 'min_data_in_leaf': 19, 'feature_fraction': 0.5748564249062494, 'bagging_fraction': 0.962106196451541, 'bagging_freq': 4, 'lambda_l1': 5.686725895019151e-06, 'lambda_l2': 3.919185969733164e-05}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,818] Trial 46 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.07768764332328308, 'num_leaves': 87, 'max_depth': 8, 'min_data_in_leaf': 23, 'feature_fraction': 0.7191223520809117, 'bagging_fraction': 0.5284697503696958, 'bagging_freq': 1, 'lambda_l1': 1.1708592108829464e-06, 'lambda_l2': 2.3463453903473945e-06}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5748564249062494, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5748564249062494\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.686725895019151e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.686725895019151e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.919185969733164e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.919185969733164e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.962106196451541, subsample=1.0 will be ignored. Current value: bagging_fraction=0.962106196451541\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5748564249062494, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5748564249062494\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.686725895019151e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.686725895019151e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.919185969733164e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.919185969733164e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.962106196451541, subsample=1.0 will be ignored. Current value: bagging_fraction=0.962106196451541\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5748564249062494, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5748564249062494\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.686725895019151e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.686725895019151e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.919185969733164e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.919185969733164e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.962106196451541, subsample=1.0 will be ignored. Current value: bagging_fraction=0.962106196451541\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.50899\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508916\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509758\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510139\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.510733\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.511689\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.511109\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.511655\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.511789\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.511794\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.512303\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.50899\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5748564249062494, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5748564249062494\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.686725895019151e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.686725895019151e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.919185969733164e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.919185969733164e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.962106196451541, subsample=1.0 will be ignored. Current value: bagging_fraction=0.962106196451541\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7191223520809117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7191223520809117\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1708592108829464e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1708592108829464e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.3463453903473945e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3463453903473945e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5284697503696958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5284697503696958\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7191223520809117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7191223520809117\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1708592108829464e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1708592108829464e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.3463453903473945e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3463453903473945e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5284697503696958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5284697503696958\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7191223520809117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7191223520809117\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1708592108829464e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1708592108829464e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.3463453903473945e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3463453903473945e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5284697503696958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5284697503696958\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508687\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508886\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509398\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.510105\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.511093\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.510811\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.511537\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.511648\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.511587\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.511605\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.511643\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508687\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=23, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7191223520809117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7191223520809117\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1708592108829464e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1708592108829464e-06\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.3463453903473945e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3463453903473945e-06\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5284697503696958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5284697503696958\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:49,948] Trial 47 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.2501514811146824, 'num_leaves': 97, 'max_depth': 4, 'min_data_in_leaf': 17, 'feature_fraction': 0.62151110302839, 'bagging_fraction': 0.5010709885295902, 'bagging_freq': 7, 'lambda_l1': 7.016041465915305e-05, 'lambda_l2': 0.029170223365601405}. Best is trial 0 with value: 0.20599999999999996.\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:50,096] Trial 48 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.1060184851988642, 'num_leaves': 76, 'max_depth': 7, 'min_data_in_leaf': 33, 'feature_fraction': 0.6380669731350772, 'bagging_fraction': 0.7865532988742769, 'bagging_freq': 6, 'lambda_l1': 3.133828952331501, 'lambda_l2': 0.44629430927223407}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62151110302839, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62151110302839\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.016041465915305e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.016041465915305e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.029170223365601405, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.029170223365601405\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5010709885295902, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5010709885295902\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62151110302839, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62151110302839\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.016041465915305e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.016041465915305e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.029170223365601405, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.029170223365601405\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5010709885295902, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5010709885295902\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62151110302839, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62151110302839\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.016041465915305e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.016041465915305e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.029170223365601405, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.029170223365601405\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5010709885295902, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5010709885295902\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.50989\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.509007\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.51134\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.511661\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.512075\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.513364\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.513927\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.51365\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.512857\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.513374\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.513812\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.50989\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62151110302839, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62151110302839\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.016041465915305e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.016041465915305e-05\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.029170223365601405, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.029170223365601405\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5010709885295902, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5010709885295902\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6380669731350772, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6380669731350772\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.133828952331501, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.133828952331501\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44629430927223407, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44629430927223407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7865532988742769, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7865532988742769\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6380669731350772, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6380669731350772\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.133828952331501, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.133828952331501\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44629430927223407, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44629430927223407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7865532988742769, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7865532988742769\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6380669731350772, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6380669731350772\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.133828952331501, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.133828952331501\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44629430927223407, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44629430927223407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7865532988742769, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7865532988742769\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508784\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508924\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.509207\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.50908\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509122\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509189\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509365\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.509809\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.509584\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.509711\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509549\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508784\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6380669731350772, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6380669731350772\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.133828952331501, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.133828952331501\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.44629430927223407, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.44629430927223407\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7865532988742769, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7865532988742769\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.5, 1.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
      "C:\\Users\\Utkarsh Rai\\AppData\\Local\\Temp\\ipykernel_12400\\1781945410.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
      "[I 2025-02-15 19:25:50,238] Trial 49 finished with value: 0.20599999999999996 and parameters: {'learning_rate': 0.03560481120716561, 'num_leaves': 71, 'max_depth': 6, 'min_data_in_leaf': 11, 'feature_fraction': 0.6907120817356843, 'bagging_fraction': 0.909132633332406, 'bagging_freq': 3, 'lambda_l1': 0.27236928409329353, 'lambda_l2': 0.0010064641150774722}. Best is trial 0 with value: 0.20599999999999996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907120817356843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907120817356843\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.27236928409329353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.27236928409329353\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0010064641150774722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0010064641150774722\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909132633332406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.909132633332406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907120817356843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907120817356843\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.27236928409329353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.27236928409329353\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0010064641150774722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0010064641150774722\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909132633332406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.909132633332406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907120817356843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907120817356843\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.27236928409329353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.27236928409329353\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0010064641150774722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0010064641150774722\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909132633332406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.909132633332406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.508654\tvalid_0's binary_error: 0.206\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.508645\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.508624\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.508706\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.509048\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.509174\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.509317\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.509305\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.509541\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.509499\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.509471\tvalid_0's binary_error: 0.206\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.508654\tvalid_0's binary_error: 0.206\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=11, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907120817356843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907120817356843\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.27236928409329353, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.27236928409329353\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0010064641150774722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0010064641150774722\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.909132633332406, subsample=1.0 will be ignored. Current value: bagging_fraction=0.909132633332406\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Best parameters: {'learning_rate': 0.17796508323626273, 'num_leaves': 27, 'max_depth': 5, 'min_data_in_leaf': 19, 'feature_fraction': 0.7416257918808059, 'bagging_fraction': 0.972406136370964, 'bagging_freq': 2, 'lambda_l1': 0.06567524024164581, 'lambda_l2': 0.1002980215328052}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d543792a-8138-45cb-a8fa-76fe22e0200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 1695, number of negative: 6305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 644\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211875 -> initscore=-1.313660\n",
      "[LightGBM] [Info] Start training from score -1.313660\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.509847\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7416257918808059, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7416257918808059\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.06567524024164581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.06567524024164581\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1002980215328052, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1002980215328052\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.972406136370964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.972406136370964\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "Model Accuracy: 0.7940\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'learning_rate': 0.17796508323626273,\n",
    "    'num_leaves': 27,\n",
    "    'max_depth': 5,\n",
    "    'min_data_in_leaf': 19,\n",
    "    'feature_fraction': 0.7416257918808059,\n",
    "    'bagging_fraction': 0.972406136370964,\n",
    "    'bagging_freq': 2,\n",
    "    'lambda_l1': 0.06567524024164581,\n",
    "    'lambda_l2': 0.1002980215328052,\n",
    "    'objective': 'binary'  # or 'multiclass' if you have more than 2 classes\n",
    "}\n",
    "\n",
    "# Train the final model with best parameters\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train, eval_metric=\"logloss\", eval_set=[(X_test, y_test)], callbacks=[lgb.early_stopping(10)])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c19440a-bf07-4b47-b726-a7ae3e630cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\utkarsh rai\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "   ---------------------------------------- 0.0/383.6 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/383.6 kB 1.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 92.2/383.6 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 194.6/383.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  378.9/383.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 383.6/383.6 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 233.6/233.6 kB 14.0 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9f43a66-a450-4479-b040-5a77cc961d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f37612bf-de28-4e1f-8a49-b86dcad35804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Define model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy  # Optuna will maximize this metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "849ddf08-34b2-4e96-bb0a-301d439cd66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 17:39:51,228] A new study created in memory with name: no-name-c46a6760-c835-48b4-9129-cfc0d5b3b73f\n",
      "[I 2025-02-15 17:39:51,854] Trial 0 finished with value: 0.794 and parameters: {'n_estimators': 55, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:39:52,939] Trial 1 finished with value: 0.794 and parameters: {'n_estimators': 85, 'max_depth': 24, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:39:56,075] Trial 2 finished with value: 0.794 and parameters: {'n_estimators': 274, 'max_depth': 28, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:39:58,352] Trial 3 finished with value: 0.794 and parameters: {'n_estimators': 277, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:00,649] Trial 4 finished with value: 0.794 and parameters: {'n_estimators': 214, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:03,335] Trial 5 finished with value: 0.794 and parameters: {'n_estimators': 204, 'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:04,607] Trial 6 finished with value: 0.794 and parameters: {'n_estimators': 115, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:09,398] Trial 7 finished with value: 0.7935 and parameters: {'n_estimators': 168, 'max_depth': 25, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:10,933] Trial 8 finished with value: 0.794 and parameters: {'n_estimators': 66, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:14,009] Trial 9 finished with value: 0.794 and parameters: {'n_estimators': 163, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:15,111] Trial 10 finished with value: 0.794 and parameters: {'n_estimators': 119, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:16,456] Trial 11 finished with value: 0.794 and parameters: {'n_estimators': 51, 'max_depth': 21, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:19,227] Trial 12 finished with value: 0.7925 and parameters: {'n_estimators': 95, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:21,494] Trial 13 finished with value: 0.794 and parameters: {'n_estimators': 86, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:25,276] Trial 14 finished with value: 0.794 and parameters: {'n_estimators': 144, 'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:26,639] Trial 15 finished with value: 0.794 and parameters: {'n_estimators': 52, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:28,213] Trial 16 finished with value: 0.794 and parameters: {'n_estimators': 89, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:31,810] Trial 17 finished with value: 0.794 and parameters: {'n_estimators': 130, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:33,973] Trial 18 finished with value: 0.794 and parameters: {'n_estimators': 201, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:35,972] Trial 19 finished with value: 0.794 and parameters: {'n_estimators': 80, 'max_depth': 24, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:40,671] Trial 20 finished with value: 0.794 and parameters: {'n_estimators': 248, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:46,473] Trial 21 finished with value: 0.794 and parameters: {'n_estimators': 293, 'max_depth': 27, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:51,140] Trial 22 finished with value: 0.794 and parameters: {'n_estimators': 241, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:56,203] Trial 23 finished with value: 0.794 and parameters: {'n_estimators': 260, 'max_depth': 27, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:58,357] Trial 24 finished with value: 0.794 and parameters: {'n_estimators': 109, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:40:59,968] Trial 25 finished with value: 0.794 and parameters: {'n_estimators': 148, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:04,353] Trial 26 finished with value: 0.794 and parameters: {'n_estimators': 182, 'max_depth': 22, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:05,951] Trial 27 finished with value: 0.794 and parameters: {'n_estimators': 68, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:10,970] Trial 28 finished with value: 0.7935 and parameters: {'n_estimators': 225, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:15,698] Trial 29 finished with value: 0.794 and parameters: {'n_estimators': 273, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:19,063] Trial 30 finished with value: 0.794 and parameters: {'n_estimators': 187, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:22,836] Trial 31 finished with value: 0.794 and parameters: {'n_estimators': 290, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:27,267] Trial 32 finished with value: 0.794 and parameters: {'n_estimators': 271, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:33,461] Trial 33 finished with value: 0.794 and parameters: {'n_estimators': 299, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:38,143] Trial 34 finished with value: 0.794 and parameters: {'n_estimators': 229, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:42,539] Trial 35 finished with value: 0.794 and parameters: {'n_estimators': 278, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:44,983] Trial 36 finished with value: 0.794 and parameters: {'n_estimators': 203, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:46,263] Trial 37 finished with value: 0.794 and parameters: {'n_estimators': 67, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:50,806] Trial 38 finished with value: 0.794 and parameters: {'n_estimators': 245, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:51,791] Trial 39 finished with value: 0.794 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:41:56,981] Trial 40 finished with value: 0.794 and parameters: {'n_estimators': 259, 'max_depth': 16, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:00,292] Trial 41 finished with value: 0.794 and parameters: {'n_estimators': 220, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:04,739] Trial 42 finished with value: 0.794 and parameters: {'n_estimators': 215, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:07,834] Trial 43 finished with value: 0.794 and parameters: {'n_estimators': 152, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:10,399] Trial 44 finished with value: 0.794 and parameters: {'n_estimators': 131, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:11,241] Trial 45 finished with value: 0.794 and parameters: {'n_estimators': 59, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:12,736] Trial 46 finished with value: 0.794 and parameters: {'n_estimators': 78, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:17,883] Trial 47 finished with value: 0.794 and parameters: {'n_estimators': 286, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:21,896] Trial 48 finished with value: 0.792 and parameters: {'n_estimators': 169, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.794.\n",
      "[I 2025-02-15 17:42:26,060] Trial 49 finished with value: 0.794 and parameters: {'n_estimators': 258, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 55, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 5}\n",
      "Best accuracy: 0.794\n"
     ]
    }
   ],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")  # We want to maximize accuracy\n",
    "study.optimize(objective, n_trials=50)  # Run 50 trials\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28d6efa5-7cfd-429d-b1d8-727096e2d1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Get feature names (if using Pandas DataFrame)\n",
    "feature_names = X_train.columns if hasattr(X_train, \"columns\") else [f\"Feature {i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(feature_importance)), feature_importance[sorted_idx], align=\"center\")\n",
    "plt.xticks(range(len(feature_importance)), np.array(feature_names)[sorted_idx], rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importance in XGBoost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6de2e-252a-4422-b44c-c69885053ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
